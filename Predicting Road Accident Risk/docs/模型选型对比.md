# è¡¨æ ¼æ•°æ®é¢„æµ‹æ¨¡å‹é€‰å‹å¯¹æ¯”

## ğŸ“‹ ç›®å½•

1. [æ¨¡å‹åˆ†ç±»æ¦‚è§ˆ](#æ¨¡å‹åˆ†ç±»æ¦‚è§ˆ)
2. [ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹](#ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹)
3. [æ·±åº¦å­¦ä¹ æ¨¡å‹](#æ·±åº¦å­¦ä¹ æ¨¡å‹)
4. [æ¨¡å‹å¯¹æ¯”æ€»ç»“](#æ¨¡å‹å¯¹æ¯”æ€»ç»“)
5. [å¦‚ä½•é€‰æ‹©æ¨¡å‹](#å¦‚ä½•é€‰æ‹©æ¨¡å‹)
6. [è‡ªå·±å®ç°æ¨¡å‹](#è‡ªå·±å®ç°æ¨¡å‹)

---

## æ¨¡å‹åˆ†ç±»æ¦‚è§ˆ

è¡¨æ ¼æ•°æ®é¢„æµ‹çš„æ¨¡å‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ å¤§ç±»ï¼š

```
è¡¨æ ¼æ•°æ®é¢„æµ‹æ¨¡å‹
â”œâ”€â”€ ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹
â”‚   â”œâ”€â”€ çº¿æ€§æ¨¡å‹ï¼ˆLinear Modelsï¼‰
â”‚   â”œâ”€â”€ æ ‘æ¨¡å‹ï¼ˆTree-based Modelsï¼‰
â”‚   â””â”€â”€ é›†æˆæ¨¡å‹ï¼ˆEnsemble Modelsï¼‰
â”‚
â”œâ”€â”€ æ·±åº¦å­¦ä¹ æ¨¡å‹
â”‚   â”œâ”€â”€ é€šç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹
â”‚   â”œâ”€â”€ ä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡çš„æ¨¡å‹
â”‚   â””â”€â”€ Transformer é£æ ¼æ¨¡å‹
â”‚
â””â”€â”€ æ··åˆæ¨¡å‹ï¼ˆHybrid Modelsï¼‰
```

---

## ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹

### 1. çº¿æ€§æ¨¡å‹

#### 1.1 çº¿æ€§å›å½’ï¼ˆLinear Regressionï¼‰
```python
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç®€å•å¿«é€Ÿï¼Œè®­ç»ƒé€Ÿåº¦å¿«
- âœ… å¯è§£é‡Šæ€§å¼ºï¼Œç³»æ•°æœ‰æ˜ç¡®å«ä¹‰
- âœ… ä¸éœ€è¦å¤ªå¤šæ•°æ®
- âœ… å†…å­˜å ç”¨å°

**ç¼ºç‚¹**ï¼š
- âŒ åªèƒ½æ•æ‰çº¿æ€§å…³ç³»
- âŒ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
- âŒ éœ€è¦ç‰¹å¾å·¥ç¨‹ï¼ˆå¦‚å¤šé¡¹å¼ç‰¹å¾ï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- ç‰¹å¾ä¸ç›®æ ‡å‘ˆçº¿æ€§å…³ç³»
- éœ€è¦å¿«é€ŸåŸºçº¿æ¨¡å‹
- æ•°æ®é‡è¾ƒå°

#### 1.2 å²­å›å½’ï¼ˆRidge Regressionï¼‰
```python
from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¤„ç†å¤šé‡å…±çº¿æ€§
- âœ… L2 æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
- âœ… è®­ç»ƒé€Ÿåº¦å¿«

**ç¼ºç‚¹**ï¼š
- âŒ ä»ç„¶åªèƒ½æ•æ‰çº¿æ€§å…³ç³»
- âŒ éœ€è¦è°ƒå‚ï¼ˆalphaï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- ç‰¹å¾ä¹‹é—´å­˜åœ¨ç›¸å…³æ€§
- éœ€è¦æ­£åˆ™åŒ–çš„çº¿æ€§æ¨¡å‹

#### 1.3 Lasso å›å½’
```python
from sklearn.linear_model import Lasso

model = Lasso(alpha=0.1)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… L1 æ­£åˆ™åŒ–ï¼Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- âœ… å¯ä»¥å¤„ç†é«˜ç»´æ•°æ®
- âœ… è®­ç»ƒé€Ÿåº¦å¿«

**ç¼ºç‚¹**ï¼š
- âŒ åªèƒ½é€‰æ‹© n ä¸ªç‰¹å¾ï¼ˆn < æ ·æœ¬æ•°ï¼‰
- âŒ ä»ç„¶åªèƒ½æ•æ‰çº¿æ€§å…³ç³»

**é€‚ç”¨åœºæ™¯**ï¼š
- ç‰¹å¾æ•°é‡å¾ˆå¤šï¼Œéœ€è¦ç‰¹å¾é€‰æ‹©
- ç¨€ç–ç‰¹å¾åœºæ™¯

### 2. æ ‘æ¨¡å‹

#### 2.1 å†³ç­–æ ‘ï¼ˆDecision Treeï¼‰
```python
from sklearn.tree import DecisionTreeRegressor

model = DecisionTreeRegressor(max_depth=10)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥æ•æ‰éçº¿æ€§å…³ç³»
- âœ… ä¸éœ€è¦ç‰¹å¾ç¼©æ”¾
- âœ… å¯è§£é‡Šæ€§å¼ºï¼ˆå¯ä»¥å¯è§†åŒ–ï¼‰
- âœ… å¤„ç†æ··åˆç±»å‹ç‰¹å¾

**ç¼ºç‚¹**ï¼š
- âŒ å®¹æ˜“è¿‡æ‹Ÿåˆ
- âŒ ä¸ç¨³å®šï¼ˆæ•°æ®å¾®å°å˜åŒ–å¯¼è‡´æ ‘ç»“æ„å˜åŒ–ï¼‰
- âŒ æ€§èƒ½é€šå¸¸ä¸å¦‚é›†æˆæ–¹æ³•

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦å¯è§£é‡Šæ€§
- ç‰¹å¾äº¤äº’å¤æ‚
- ä½œä¸ºåŸºçº¿æ¨¡å‹

#### 2.2 éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰
```python
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    random_state=42
)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¼ºå¤§çš„éçº¿æ€§å»ºæ¨¡èƒ½åŠ›
- âœ… è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- âœ… å¯¹è¿‡æ‹Ÿåˆæœ‰è¾ƒå¥½çš„æŠµæŠ—
- âœ… å¯ä»¥å¤„ç†ç¼ºå¤±å€¼
- âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆå¯å¹¶è¡Œï¼‰
- âœ… æä¾›ç‰¹å¾é‡è¦æ€§

**ç¼ºç‚¹**ï¼š
- âŒ å†…å­˜å ç”¨è¾ƒå¤§ï¼ˆæ ‘çš„æ•°é‡å¤šï¼‰
- âŒ é¢„æµ‹é€Ÿåº¦è¾ƒæ…¢ï¼ˆéœ€è¦éå†æ‰€æœ‰æ ‘ï¼‰
- âŒ å¯¹å™ªå£°æ•æ„Ÿ

**é€‚ç”¨åœºæ™¯**ï¼š
- **æœ€å¸¸ç”¨çš„è¡¨æ ¼æ•°æ®æ¨¡å‹ä¹‹ä¸€**
- ç‰¹å¾æ•°é‡ä¸­ç­‰ï¼ˆ< 1000ï¼‰
- éœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æ
- æ•°æ®é‡ä¸­ç­‰ï¼ˆ< 1M æ ·æœ¬ï¼‰

#### 2.3 æ¢¯åº¦æå‡æ ‘ï¼ˆGradient Boostingï¼‰

##### 2.3.1 XGBoost
```python
import xgboost as xgb

model = xgb.XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **Kaggle ç«èµ›ä¸­æœ€å¸¸ç”¨çš„æ¨¡å‹**
- âœ… æ€§èƒ½ä¼˜ç§€ï¼Œé€šå¸¸æ¯”éšæœºæ£®æ—æ›´å¥½
- âœ… è®­ç»ƒé€Ÿåº¦å¿«ï¼ˆå¹¶è¡Œä¼˜åŒ–ï¼‰
- âœ… å†…ç½®æ­£åˆ™åŒ–
- âœ… å¤„ç†ç¼ºå¤±å€¼
- âœ… æä¾›ç‰¹å¾é‡è¦æ€§

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦è°ƒå‚ï¼ˆå­¦ä¹ ç‡ã€æ·±åº¦ç­‰ï¼‰
- âŒ å†…å­˜å ç”¨è¾ƒå¤§
- âŒ å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ

**é€‚ç”¨åœºæ™¯**ï¼š
- **è¡¨æ ¼æ•°æ®é¢„æµ‹çš„é¦–é€‰æ¨¡å‹ä¹‹ä¸€**
- ç«èµ›å’Œå®é™…é¡¹ç›®
- éœ€è¦é«˜æ€§èƒ½
- æ•°æ®é‡è¾ƒå¤§

##### 2.3.2 LightGBM
```python
import lightgbm as lgb

model = lgb.LGBMRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **è®­ç»ƒé€Ÿåº¦æœ€å¿«**ï¼ˆæ¯” XGBoost å¿«ï¼‰
- âœ… å†…å­˜å ç”¨å°
- âœ… æ€§èƒ½ä¸ XGBoost ç›¸å½“æˆ–æ›´å¥½
- âœ… å¤„ç†å¤§è§„æ¨¡æ•°æ®
- âœ… å†…ç½®ç±»åˆ«ç‰¹å¾å¤„ç†

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦è°ƒå‚
- âŒ å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿï¼ˆéœ€è¦æ—©åœï¼‰

**é€‚ç”¨åœºæ™¯**ï¼š
- **å¤§è§„æ¨¡æ•°æ®ï¼ˆ> 1M æ ·æœ¬ï¼‰**
- éœ€è¦å¿«é€Ÿè®­ç»ƒ
- å†…å­˜å—é™ç¯å¢ƒ
- ç±»åˆ«ç‰¹å¾å¤š

##### 2.3.3 CatBoost
```python
from catboost import CatBoostRegressor

model = CatBoostRegressor(
    iterations=100,
    depth=6,
    learning_rate=0.1,
    random_seed=42,
    verbose=False
)
model.fit(X_train, y_train, cat_features=CATS)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **è‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾**ï¼ˆæ— éœ€ç¼–ç ï¼‰
- âœ… å¯¹è¿‡æ‹Ÿåˆæœ‰å¾ˆå¥½çš„æŠµæŠ—
- âœ… æ€§èƒ½ä¼˜ç§€
- âœ… é»˜è®¤å‚æ•°é€šå¸¸å°±å¾ˆå¥½

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼ˆæ¯” LightGBM æ…¢ï¼‰
- âŒ å†…å­˜å ç”¨è¾ƒå¤§

**é€‚ç”¨åœºæ™¯**ï¼š
- **ç±»åˆ«ç‰¹å¾å¾ˆå¤š**
- éœ€è¦è¾ƒå°‘è°ƒå‚
- å¯¹è¿‡æ‹Ÿåˆæ•æ„Ÿçš„åœºæ™¯

### 3. é›†æˆæ¨¡å‹

#### 3.1 Stacking
```python
from sklearn.ensemble import StackingRegressor
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

base_models = [
    ('rf', RandomForestRegressor(n_estimators=100)),
    ('xgb', xgb.XGBRegressor(n_estimators=100)),
]

model = StackingRegressor(
    estimators=base_models,
    final_estimator=Ridge()
)
model.fit(X_train, y_train)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç»“åˆå¤šä¸ªæ¨¡å‹çš„ä¼˜åŠ¿
- âœ… é€šå¸¸æ€§èƒ½æ›´å¥½
- âœ… å‡å°‘å•ä¸ªæ¨¡å‹çš„åå·®

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ å¤æ‚åº¦é«˜
- âŒ éœ€è¦æ›´å¤šæ•°æ®

**é€‚ç”¨åœºæ™¯**ï¼š
- ç«èµ›ä¸­è¿½æ±‚æè‡´æ€§èƒ½
- æœ‰å……è¶³çš„è®¡ç®—èµ„æº
- å¤šä¸ªæ¨¡å‹æ€§èƒ½æ¥è¿‘

---

## æ·±åº¦å­¦ä¹ æ¨¡å‹

### 1. é€šç”¨æ·±åº¦å­¦ä¹ æ¨¡å‹

#### 1.1 å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰
```python
import torch
import torch.nn as nn

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims=[128, 64], dropout=0.2):
        super().__init__()
        layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥æ•æ‰éçº¿æ€§å…³ç³»
- âœ… çµæ´»çš„ç½‘ç»œç»“æ„
- âœ… å¯ä»¥ä½¿ç”¨ GPU åŠ é€Ÿ

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦å¤§é‡æ•°æ®
- âŒ å¯¹ç‰¹å¾å·¥ç¨‹æ•æ„Ÿ
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ åœ¨è¡¨æ ¼æ•°æ®ä¸Šé€šå¸¸ä¸å¦‚æ ‘æ¨¡å‹

**é€‚ç”¨åœºæ™¯**ï¼š
- æ•°æ®é‡å¾ˆå¤§ï¼ˆ> 1M æ ·æœ¬ï¼‰
- ç‰¹å¾æ•°é‡å¾ˆå¤šï¼ˆ> 1000ï¼‰
- æœ‰ GPU èµ„æº

#### 1.2 æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰
```python
class ResBlock(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.linear1 = nn.Linear(dim, dim)
        self.linear2 = nn.Linear(dim, dim)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        residual = x
        out = self.relu(self.linear1(x))
        out = self.linear2(out)
        return self.relu(out + residual)

class ResNet(nn.Module):
    def __init__(self, input_dim, hidden_dim=128, num_blocks=3):
        super().__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        self.blocks = nn.ModuleList([ResBlock(hidden_dim) for _ in range(num_blocks)])
        self.output_layer = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        x = self.input_layer(x)
        for block in self.blocks:
            x = block(x)
        return self.output_layer(x)
```

**ä¼˜ç‚¹**ï¼š
- âœ… å¯ä»¥è®­ç»ƒæ›´æ·±çš„ç½‘ç»œ
- âœ… ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
- âœ… æ€§èƒ½é€šå¸¸æ¯”æ™®é€š MLP å¥½

**ç¼ºç‚¹**ï¼š
- âŒ ä»ç„¶éœ€è¦å¤§é‡æ•°æ®
- âŒ åœ¨è¡¨æ ¼æ•°æ®ä¸Šé€šå¸¸ä¸å¦‚æ ‘æ¨¡å‹

### 2. ä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡çš„æ¨¡å‹

#### 2.1 TabNet
```python
from pytorch_tabnet.tab_model import TabNetRegressor

model = TabNetRegressor(
    n_d=64,
    n_a=64,
    n_steps=5,
    gamma=1.5,
    lambda_sparse=1e-3,
    optimizer_fn=torch.optim.Adam,
    optimizer_params=dict(lr=2e-2),
    mask_type='entmax'
)
model.fit(
    X_train.values, y_train.values,
    eval_set=[(X_val.values, y_val.values)],
    max_epochs=100,
    patience=10
)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **ä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡**
- âœ… **å¯è§£é‡Šæ€§å¼º**ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰
- âœ… è‡ªåŠ¨ç‰¹å¾é€‰æ‹©
- âœ… å¤„ç†æ··åˆç±»å‹ç‰¹å¾
- âœ… æ€§èƒ½ä¼˜ç§€

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ éœ€è¦è°ƒå‚
- âŒ å†…å­˜å ç”¨è¾ƒå¤§

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦å¯è§£é‡Šæ€§
- ç‰¹å¾æ•°é‡ä¸­ç­‰
- æœ‰ GPU èµ„æº

#### 2.2 TabM (Tabular Model) - å½“å‰ä½¿ç”¨çš„æ¨¡å‹
```python
from pytabkit import TabM_D_Regressor

model = TabM_D_Regressor(
    arch_type='tabm-mini',
    tabm_k=32,
    n_blocks=5,
    d_block=432,
    lr=0.000624,
    # ... å…¶ä»–å‚æ•°
)
model.fit(X_train, y_train, X_val, y_val, cat_col_names=CATS)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **ä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®ä¼˜åŒ–**
- âœ… **Transformer é£æ ¼çš„æ³¨æ„åŠ›æœºåˆ¶**
- âœ… å¤„ç†æ··åˆç±»å‹ç‰¹å¾ï¼ˆæ•°å€¼+åˆ†ç±»ï¼‰
- âœ… æ•°å€¼åµŒå…¥ï¼ˆPWLï¼‰å¤„ç†è¿ç»­ç‰¹å¾
- âœ… è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’
- âœ… æ€§èƒ½ä¼˜ç§€

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ éœ€è¦ GPUï¼ˆCPU è®­ç»ƒå¾ˆæ…¢ï¼‰
- âŒ å†…å­˜å ç”¨è¾ƒå¤§
- âŒ éœ€è¦è°ƒå‚

**é€‚ç”¨åœºæ™¯**ï¼š
- **å½“å‰é¡¹ç›®ä½¿ç”¨çš„æ¨¡å‹**
- éœ€è¦æ•æ‰å¤æ‚ç‰¹å¾äº¤äº’
- æœ‰ GPU èµ„æº
- è¿½æ±‚é«˜æ€§èƒ½

#### 2.3 FT-Transformer
```python
# ä½¿ç”¨ rtdl åº“
from rtdl import FTTransformer

model = FTTransformer(
    n_num_features=len(NUMERIC_FEATURES),
    cat_cardinalities=[len(cat) for cat in CATEGORICAL_FEATURES],
    d_token=192,
    n_blocks=3,
    attention_dropout=0.1,
    ffn_d_hidden=768,
    ffn_dropout=0.1,
    residual_dropout=0.1,
)
```

**ä¼˜ç‚¹**ï¼š
- âœ… **åŸºäº Transformer æ¶æ„**
- âœ… æ€§èƒ½ä¼˜ç§€
- âœ… å¤„ç†æ··åˆç±»å‹ç‰¹å¾
- âœ… è‡ªåŠ¨å­¦ä¹ ç‰¹å¾äº¤äº’

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ éœ€è¦ GPU
- âŒ å†…å­˜å ç”¨å¤§

**é€‚ç”¨åœºæ™¯**ï¼š
- éœ€è¦ Transformer é£æ ¼çš„æ¨¡å‹
- æœ‰ GPU èµ„æº
- è¿½æ±‚é«˜æ€§èƒ½

#### 2.4 NODE (Neural Oblivious Decision Ensembles)
```python
from pytorch_tabular import TabularModel
from pytorch_tabular.models import NodeConfig

config = NodeConfig(
    task='regression',
    target=['accident_risk'],
    continuous_cols=NUMERIC_FEATURES,
    categorical_cols=CATEGORICAL_FEATURES,
    num_layers=4,
    num_trees=2048,
    depth=6,
    learning_rate=0.001,
)
model = TabularModel(config)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ç»“åˆæ ‘æ¨¡å‹å’Œç¥ç»ç½‘ç»œ
- âœ… æ€§èƒ½ä¼˜ç§€
- âœ… å¯è§£é‡Šæ€§è¾ƒå¥½

**ç¼ºç‚¹**ï¼š
- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ éœ€è¦ GPU

---

## æ¨¡å‹å¯¹æ¯”æ€»ç»“

### æ€§èƒ½å¯¹æ¯”ï¼ˆä¸€èˆ¬æƒ…å†µï¼‰

| æ¨¡å‹ç±»å‹ | æ€§èƒ½ | è®­ç»ƒé€Ÿåº¦ | å†…å­˜å ç”¨ | å¯è§£é‡Šæ€§ | æ•°æ®éœ€æ±‚ |
|---------|------|---------|---------|---------|---------|
| **çº¿æ€§å›å½’** | â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **éšæœºæ£®æ—** | â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­â­ | â­â­â­ |
| **XGBoost** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |
| **LightGBM** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ | â­â­â­ |
| **CatBoost** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­ |
| **MLP** | â­â­â­ | â­â­â­ | â­â­â­ | â­â­ | â­â­ |
| **TabNet** | â­â­â­â­ | â­â­ | â­â­ | â­â­â­â­ | â­â­â­ |
| **TabM** | â­â­â­â­â­ | â­â­ | â­â­ | â­â­â­ | â­â­â­ |
| **FT-Transformer** | â­â­â­â­â­ | â­â­ | â­â­ | â­â­ | â­â­â­ |

### è¯¦ç»†å¯¹æ¯”è¡¨

| ç‰¹æ€§ | TabM | XGBoost | LightGBM | CatBoost | TabNet | MLP |
|------|------|---------|----------|----------|--------|-----|
| **è¡¨æ ¼æ•°æ®ä¼˜åŒ–** | âœ… ä¸“é—¨è®¾è®¡ | âœ… ä¼˜ç§€ | âœ… ä¼˜ç§€ | âœ… ä¼˜ç§€ | âœ… ä¸“é—¨è®¾è®¡ | âŒ é€šç”¨ |
| **ç‰¹å¾äº¤äº’** | âœ… è‡ªåŠ¨å­¦ä¹  | âœ… æ ‘ç»“æ„ | âœ… æ ‘ç»“æ„ | âœ… æ ‘ç»“æ„ | âœ… æ³¨æ„åŠ› | âš ï¸ éœ€è¦è®¾è®¡ |
| **æ··åˆç‰¹å¾** | âœ… åŸç”Ÿæ”¯æŒ | âš ï¸ éœ€è¦ç¼–ç  | âš ï¸ éœ€è¦ç¼–ç  | âœ… åŸç”Ÿæ”¯æŒ | âš ï¸ éœ€è¦ç¼–ç  | âš ï¸ éœ€è¦ç¼–ç  |
| **è®­ç»ƒé€Ÿåº¦** | â­â­ æ…¢ | â­â­â­â­ å¿« | â­â­â­â­â­ å¾ˆå¿« | â­â­â­ ä¸­ç­‰ | â­â­ æ…¢ | â­â­â­ ä¸­ç­‰ |
| **GPU æ”¯æŒ** | âœ… å¿…éœ€ | âš ï¸ å¯é€‰ | âš ï¸ å¯é€‰ | âš ï¸ å¯é€‰ | âœ… æ¨è | âœ… æ¨è |
| **å†…å­˜å ç”¨** | â­â­ å¤§ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å° | â­â­â­ ä¸­ç­‰ | â­â­ å¤§ | â­â­â­ ä¸­ç­‰ |
| **å¯è§£é‡Šæ€§** | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ å¥½ | â­â­ å·® |
| **è°ƒå‚éš¾åº¦** | â­â­ éš¾ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­â­ ç®€å• | â­â­ éš¾ | â­â­â­ ä¸­ç­‰ |
| **æ•°æ®éœ€æ±‚** | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­â­ ä¸­ç­‰ | â­â­ å¤§ |

### TabM vs å…¶ä»–æ¨¡å‹çš„åŒºåˆ«

#### TabM vs XGBoost/LightGBM

| æ–¹é¢ | TabM | XGBoost/LightGBM |
|------|------|------------------|
| **æ¶æ„** | æ·±åº¦å­¦ä¹ ï¼ˆTransformerï¼‰ | æ¢¯åº¦æå‡æ ‘ |
| **ç‰¹å¾å¤„ç†** | åµŒå…¥å±‚ï¼ˆæ•°å€¼+åˆ†ç±»ï¼‰ | æ ‘åˆ†è£‚ï¼ˆéœ€è¦ç¼–ç ï¼‰ |
| **ç‰¹å¾äº¤äº’** | æ³¨æ„åŠ›æœºåˆ¶è‡ªåŠ¨å­¦ä¹  | æ ‘ç»“æ„éšå¼å­¦ä¹  |
| **è®­ç»ƒ** | éœ€è¦ GPUï¼Œè®­ç»ƒæ…¢ | CPU/GPU éƒ½å¯ï¼Œè®­ç»ƒå¿« |
| **æ€§èƒ½** | é€šå¸¸ç•¥å¥½ï¼ˆå¤æ‚äº¤äº’ï¼‰ | é€šå¸¸å¾ˆå¥½ï¼ˆç®€å•äº¤äº’ï¼‰ |
| **å¯è§£é‡Šæ€§** | æ³¨æ„åŠ›æƒé‡ | ç‰¹å¾é‡è¦æ€§ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚ç‰¹å¾äº¤äº’ï¼Œæœ‰ GPU | å¿«é€Ÿè¿­ä»£ï¼Œé€šç”¨åœºæ™¯ |

#### TabM vs TabNet

| æ–¹é¢ | TabM | TabNet |
|------|------|--------|
| **æ¶æ„** | Transformer é£æ ¼ | æ³¨æ„åŠ› + å†³ç­–æ­¥éª¤ |
| **ç‰¹å¾åµŒå…¥** | PWL æ•°å€¼åµŒå…¥ | åŸå§‹ç‰¹å¾ |
| **å¯è§£é‡Šæ€§** | æ³¨æ„åŠ›æƒé‡ | ç‰¹å¾é€‰æ‹©æ©ç  |
| **æ€§èƒ½** | é€šå¸¸æ›´å¥½ | å¾ˆå¥½ |
| **è®­ç»ƒé€Ÿåº¦** | è¾ƒæ…¢ | è¾ƒæ…¢ |

---

## å¦‚ä½•é€‰æ‹©æ¨¡å‹

### å†³ç­–æ ‘

```
å¼€å§‹
  â”‚
  â”œâ”€ æ•°æ®é‡ < 10Kï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šéšæœºæ£®æ— æˆ– XGBoost
  â”‚
  â”œâ”€ éœ€è¦å¿«é€ŸåŸºçº¿ï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šçº¿æ€§å›å½’ æˆ– éšæœºæ£®æ—
  â”‚
  â”œâ”€ ç±»åˆ«ç‰¹å¾å¾ˆå¤šï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šCatBoost
  â”‚
  â”œâ”€ æ•°æ®é‡ > 1Mï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šLightGBM
  â”‚
  â”œâ”€ æœ‰ GPU ä¸”è¿½æ±‚æè‡´æ€§èƒ½ï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šTabM æˆ– FT-Transformer
  â”‚
  â”œâ”€ éœ€è¦å¯è§£é‡Šæ€§ï¼Ÿ
  â”‚   â””â”€> ä½¿ç”¨ï¼šTabNet æˆ– éšæœºæ£®æ—
  â”‚
  â””â”€ é»˜è®¤é€‰æ‹©
      â””â”€> ä½¿ç”¨ï¼šXGBoost æˆ– LightGBM
```

### å…·ä½“å»ºè®®

#### 1. **å¿«é€ŸåŸå‹å¼€å‘**
- **æ¨è**ï¼šLightGBM æˆ– XGBoost
- **åŸå› **ï¼šè®­ç»ƒå¿«ï¼Œæ€§èƒ½å¥½ï¼Œè°ƒå‚ç®€å•

#### 2. **ç«èµ›è¿½æ±‚æè‡´æ€§èƒ½**
- **æ¨è**ï¼šTabM + XGBoost + LightGBM é›†æˆ
- **åŸå› **ï¼šç»“åˆæ·±åº¦å­¦ä¹ å’Œæ ‘æ¨¡å‹çš„ä¼˜åŠ¿

#### 3. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**
- **æ¨è**ï¼šLightGBM æˆ– XGBoost
- **åŸå› **ï¼šè®­ç»ƒå¿«ï¼Œé¢„æµ‹å¿«ï¼Œç¨³å®šæ€§å¥½

#### 4. **éœ€è¦å¯è§£é‡Šæ€§**
- **æ¨è**ï¼šTabNet æˆ– éšæœºæ£®æ—
- **åŸå› **ï¼šæä¾›ç‰¹å¾é‡è¦æ€§æˆ–æ³¨æ„åŠ›æƒé‡

#### 5. **ç±»åˆ«ç‰¹å¾å¾ˆå¤š**
- **æ¨è**ï¼šCatBoost
- **åŸå› **ï¼šè‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾ï¼Œæ— éœ€ç¼–ç 

#### 6. **æœ‰ GPU èµ„æº**
- **æ¨è**ï¼šTabM æˆ– FT-Transformer
- **åŸå› **ï¼šå¯ä»¥åˆ©ç”¨ GPU åŠ é€Ÿï¼Œæ€§èƒ½æ›´å¥½

---

## è‡ªå·±å®ç°æ¨¡å‹

### 1. ç®€å•çš„ç¥ç»ç½‘ç»œå›å½’å™¨

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder

class TabularDataset(Dataset):
    def __init__(self, X, y=None):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y) if y is not None else None
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        if self.y is not None:
            return self.X[idx], self.y[idx]
        return self.X[idx]

class SimpleTabularNN(nn.Module):
    """ç®€å•çš„è¡¨æ ¼æ•°æ®ç¥ç»ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dims=[256, 128, 64], dropout=0.2):
        super().__init__()
        layers = []
        prev_dim = input_dim
        
        for hidden_dim in hidden_dims:
            layers.append(nn.Linear(prev_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(dropout))
            prev_dim = hidden_dim
        
        layers.append(nn.Linear(prev_dim, 1))
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x).squeeze()

def train_model(model, train_loader, val_loader, epochs=100, lr=0.001, device='cuda'):
    """è®­ç»ƒæ¨¡å‹"""
    model = model.to(device)
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=10)
    
    best_val_loss = float('inf')
    patience = 20
    patience_counter = 0
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            pred = model(X_batch)
            loss = criterion(pred, y_batch)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        
        # éªŒè¯
        model.eval()
        val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                pred = model(X_batch)
                loss = criterion(pred, y_batch)
                val_loss += loss.item()
        
        train_loss /= len(train_loader)
        val_loss /= len(val_loader)
        
        scheduler.step(val_loss)
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
        else:
            patience_counter += 1
        
        if (epoch + 1) % 10 == 0:
            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}')
        
        if patience_counter >= patience:
            print(f'Early stopping at epoch {epoch+1}')
            break
    
    return model

# ä½¿ç”¨ç¤ºä¾‹
def run_custom_model(X_train, y_train, X_val, y_val, X_test):
    # æ•°æ®é¢„å¤„ç†
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    X_test_scaled = scaler.transform(X_test)
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = TabularDataset(X_train_scaled, y_train.values)
    val_dataset = TabularDataset(X_val_scaled, y_val.values)
    test_dataset = TabularDataset(X_test_scaled)
    
    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model = SimpleTabularNN(input_dim=X_train.shape[1], hidden_dims=[256, 128, 64])
    
    # è®­ç»ƒ
    model = train_model(model, train_loader, val_loader, epochs=100, device=device)
    
    # é¢„æµ‹
    model.eval()
    predictions = []
    with torch.no_grad():
        for X_batch in test_loader:
            X_batch = X_batch.to(device)
            pred = model(X_batch)
            predictions.append(pred.cpu().numpy())
    
    return np.concatenate(predictions)
```

### 2. å¸¦æ®‹å·®è¿æ¥çš„ç¥ç»ç½‘ç»œ

```python
class ResidualBlock(nn.Module):
    """æ®‹å·®å—"""
    def __init__(self, dim, dropout=0.2):
        super().__init__()
        self.linear1 = nn.Linear(dim, dim)
        self.bn1 = nn.BatchNorm1d(dim)
        self.linear2 = nn.Linear(dim, dim)
        self.bn2 = nn.BatchNorm1d(dim)
        self.dropout = nn.Dropout(dropout)
        self.relu = nn.ReLU()
    
    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.linear1(x)))
        out = self.dropout(out)
        out = self.bn2(self.linear2(out))
        out += residual  # æ®‹å·®è¿æ¥
        return self.relu(out)

class ResidualTabularNN(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„è¡¨æ ¼æ•°æ®ç¥ç»ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dim=128, num_blocks=3, dropout=0.2):
        super().__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        self.bn_input = nn.BatchNorm1d(hidden_dim)
        self.relu = nn.ReLU()
        
        self.blocks = nn.ModuleList([
            ResidualBlock(hidden_dim, dropout) for _ in range(num_blocks)
        ])
        
        self.output_layer = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        x = self.relu(self.bn_input(self.input_layer(x)))
        for block in self.blocks:
            x = block(x)
        return self.output_layer(x).squeeze()
```

### 3. å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„æ¨¡å‹

```python
class AttentionBlock(nn.Module):
    """æ³¨æ„åŠ›å—"""
    def __init__(self, dim, num_heads=4):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.query = nn.Linear(dim, dim)
        self.key = nn.Linear(dim, dim)
        self.value = nn.Linear(dim, dim)
        self.output = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim)
    
    def forward(self, x):
        B, N, D = x.shape
        residual = x
        
        # è‡ªæ³¨æ„åŠ›
        Q = self.query(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        K = self.key(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        V = self.value(x).view(B, N, self.num_heads, self.head_dim).transpose(1, 2)
        
        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.head_dim)
        attn = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn, V)
        
        out = out.transpose(1, 2).contiguous().view(B, N, D)
        out = self.output(out)
        out = self.norm(out + residual)
        
        return out

class AttentionTabularNN(nn.Module):
    """å¸¦æ³¨æ„åŠ›æœºåˆ¶çš„è¡¨æ ¼æ•°æ®ç¥ç»ç½‘ç»œ"""
    def __init__(self, input_dim, hidden_dim=128, num_blocks=3, num_heads=4):
        super().__init__()
        self.input_layer = nn.Linear(input_dim, hidden_dim)
        
        # å°†ç‰¹å¾è½¬æ¢ä¸ºåºåˆ—ï¼ˆæ¯ä¸ªç‰¹å¾æ˜¯ä¸€ä¸ª tokenï¼‰
        self.feature_embedding = nn.Linear(1, hidden_dim)
        
        self.blocks = nn.ModuleList([
            AttentionBlock(hidden_dim, num_heads) for _ in range(num_blocks)
        ])
        
        self.output_layer = nn.Linear(hidden_dim, 1)
    
    def forward(self, x):
        # x: (batch_size, num_features)
        B, N = x.shape
        x = x.unsqueeze(-1)  # (batch_size, num_features, 1)
        x = self.feature_embedding(x)  # (batch_size, num_features, hidden_dim)
        
        for block in self.blocks:
            x = block(x)
        
        # å…¨å±€å¹³å‡æ± åŒ–
        x = x.mean(dim=1)  # (batch_size, hidden_dim)
        return self.output_layer(x).squeeze()
```

### 4. å®Œæ•´çš„è®­ç»ƒè„šæœ¬ç¤ºä¾‹

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import root_mean_squared_error
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

# ä½¿ç”¨ä¸Šé¢çš„ SimpleTabularNN ç±»

def train_custom_model_with_cv(X, y, X_test, n_splits=5):
    """ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒè‡ªå®šä¹‰æ¨¡å‹"""
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    oof_preds = np.zeros(len(X))
    test_preds = np.zeros(len(X_test))
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_test_scaled = scaler.transform(X_test)
    
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f'Using device: {device}')
    
    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):
        print(f'\n--- Fold {fold+1}/{n_splits} ---')
        
        X_train_fold = X_scaled[train_idx]
        X_val_fold = X_scaled[val_idx]
        y_train_fold = y.iloc[train_idx].values
        y_val_fold = y.iloc[val_idx].values
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = TabularDataset(X_train_fold, y_train_fold)
        val_dataset = TabularDataset(X_val_fold, y_val_fold)
        test_dataset = TabularDataset(X_test_scaled)
        
        train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)
        
        # åˆ›å»ºæ¨¡å‹
        model = SimpleTabularNN(
            input_dim=X.shape[1],
            hidden_dims=[256, 128, 64],
            dropout=0.2
        )
        
        # è®­ç»ƒ
        model = train_model(
            model, train_loader, val_loader,
            epochs=100, lr=0.001, device=device
        )
        
        # éªŒè¯é›†é¢„æµ‹
        model.eval()
        val_preds = []
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch = X_batch.to(device)
                pred = model(X_batch)
                val_preds.append(pred.cpu().numpy())
        oof_preds[val_idx] = np.concatenate(val_preds)
        
        # æµ‹è¯•é›†é¢„æµ‹
        test_preds_fold = []
        with torch.no_grad():
            for X_batch in test_loader:
                X_batch = X_batch.to(device)
                pred = model(X_batch)
                test_preds_fold.append(pred.cpu().numpy())
        test_preds += np.concatenate(test_preds_fold)
        
        # è®¡ç®— RMSE
        rmse = root_mean_squared_error(y_val_fold, oof_preds[val_idx])
        print(f'Fold {fold+1} RMSE: {rmse:.5f}')
    
    test_preds /= n_splits
    overall_rmse = root_mean_squared_error(y, oof_preds)
    print(f'\nOverall OOF RMSE: {overall_rmse:.5f}')
    
    return oof_preds, test_preds
```

### 5. æ¨¡å‹æ”¹è¿›å»ºè®®

#### 5.1 ç‰¹å¾åµŒå…¥
```python
class FeatureEmbedding(nn.Module):
    """ç‰¹å¾åµŒå…¥å±‚"""
    def __init__(self, num_numeric, num_categorical, embed_dim=64):
        super().__init__()
        self.numeric_layer = nn.Linear(num_numeric, embed_dim)
        self.categorical_embeddings = nn.ModuleList([
            nn.Embedding(cardinality, embed_dim)
            for cardinality in num_categorical
        ])
    
    def forward(self, numeric_features, categorical_features):
        # æ•°å€¼ç‰¹å¾
        numeric_emb = self.numeric_layer(numeric_features)
        
        # åˆ†ç±»ç‰¹å¾
        cat_embs = []
        for i, emb in enumerate(self.categorical_embeddings):
            cat_embs.append(emb(categorical_features[:, i]))
        cat_emb = torch.stack(cat_embs, dim=1).sum(dim=1)
        
        # æ‹¼æ¥
        return torch.cat([numeric_emb, cat_emb], dim=1)
```

#### 5.2 æ•°å€¼ç‰¹å¾åˆ†ç®±
```python
class NumericBinning(nn.Module):
    """æ•°å€¼ç‰¹å¾åˆ†ç®±ï¼ˆç±»ä¼¼ TabM çš„ PWLï¼‰"""
    def __init__(self, num_features, n_bins=32, embed_dim=64):
        super().__init__()
        self.n_bins = n_bins
        self.embeddings = nn.ModuleList([
            nn.Embedding(n_bins, embed_dim) for _ in range(num_features)
        ])
        self.bin_edges = nn.ParameterList([
            nn.Parameter(torch.linspace(0, 1, n_bins-1)) for _ in range(num_features)
        ])
    
    def forward(self, x):
        # x: (batch_size, num_features)
        batch_size, num_features = x.shape
        embedded = []
        
        for i in range(num_features):
            # å½’ä¸€åŒ–åˆ° [0, 1]
            x_norm = (x[:, i] - x[:, i].min()) / (x[:, i].max() - x[:, i].min() + 1e-8)
            
            # æ‰¾åˆ°å¯¹åº”çš„ bin
            bins = torch.bucketize(x_norm, self.bin_edges[i])
            bins = torch.clamp(bins, 0, self.n_bins - 1)
            
            # åµŒå…¥
            embedded.append(self.embeddings[i](bins))
        
        return torch.stack(embedded, dim=1)  # (batch_size, num_features, embed_dim)
```

---

## æ€»ç»“

### æ¨¡å‹é€‰æ‹©å»ºè®®

1. **å¿«é€Ÿå¼€å§‹**ï¼šä½¿ç”¨ LightGBM æˆ– XGBoost
2. **è¿½æ±‚æ€§èƒ½**ï¼šä½¿ç”¨ TabM æˆ– FT-Transformerï¼ˆéœ€è¦ GPUï¼‰
3. **éœ€è¦å¯è§£é‡Šæ€§**ï¼šä½¿ç”¨ TabNet æˆ–éšæœºæ£®æ—
4. **ç±»åˆ«ç‰¹å¾å¤š**ï¼šä½¿ç”¨ CatBoost
5. **è‡ªå·±å®ç°**ï¼šä»ç®€å•çš„ MLP å¼€å§‹ï¼Œé€æ­¥æ·»åŠ æ®‹å·®è¿æ¥ã€æ³¨æ„åŠ›æœºåˆ¶ç­‰

### è‡ªå·±å®ç°æ¨¡å‹çš„ä¼˜åŠ¿

1. **å®Œå…¨æ§åˆ¶**ï¼šå¯ä»¥è‡ªå®šä¹‰æ¶æ„å’Œè®­ç»ƒè¿‡ç¨‹
2. **å­¦ä¹ ä»·å€¼**ï¼šæ·±å…¥ç†è§£æ¨¡å‹åŸç†
3. **å®šåˆ¶åŒ–**ï¼šé’ˆå¯¹ç‰¹å®šé—®é¢˜ä¼˜åŒ–
4. **çµæ´»æ€§**ï¼šå¯ä»¥å°è¯•æ–°çš„æƒ³æ³•

### è‡ªå·±å®ç°æ¨¡å‹çš„æŒ‘æˆ˜

1. **å¼€å‘æ—¶é—´é•¿**ï¼šéœ€è¦å®ç°å’Œè°ƒè¯•
2. **æ€§èƒ½å¯èƒ½ä¸å¦‚æˆç†Ÿåº“**ï¼šéœ€è¦å¤§é‡ä¼˜åŒ–
3. **éœ€è¦æ·±åº¦å­¦ä¹ çŸ¥è¯†**ï¼šéœ€è¦ç†è§£å„ç§æŠ€æœ¯

### æ¨èè·¯å¾„

1. **åˆå­¦è€…**ï¼šå…ˆä½¿ç”¨æˆç†Ÿçš„åº“ï¼ˆXGBoost, LightGBMï¼‰
2. **è¿›é˜¶**ï¼šå°è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ˆTabM, TabNetï¼‰
3. **é«˜çº§**ï¼šè‡ªå·±å®ç°æ¨¡å‹ï¼Œç»“åˆå¤šç§æŠ€æœ¯

---

## å‚è€ƒèµ„æ–™

- **XGBoost**: https://xgboost.readthedocs.io/
- **LightGBM**: https://lightgbm.readthedocs.io/
- **CatBoost**: https://catboost.ai/
- **TabNet**: https://github.com/dreamquark-ai/tabnet
- **TabM**: https://github.com/georgian-io/pytabkit
- **FT-Transformer**: https://github.com/yandex-research/rtdl

