# TabM_D_Regressor å®ç°åŸç†ä¸ä»£ç åˆ†æ

## ğŸ“‹ ç›®å½•

1. [TabM æ¦‚è¿°](#tabm-æ¦‚è¿°)
2. [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
3. [å…³é”®ç»„ä»¶å®ç°](#å…³é”®ç»„ä»¶å®ç°)
4. [å®Œæ•´å®ç°ä»£ç ](#å®Œæ•´å®ç°ä»£ç )
5. [è®­ç»ƒæµç¨‹åˆ†æ](#è®­ç»ƒæµç¨‹åˆ†æ)
6. [å…³é”®æŠ€æœ¯è¯¦è§£](#å…³é”®æŠ€æœ¯è¯¦è§£)

---

## TabM æ¦‚è¿°

**TabM (Tabular Model)** æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºè¡¨æ ¼æ•°æ®è®¾è®¡çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç»“åˆäº† Transformer æ¶æ„å’Œè¡¨æ ¼æ•°æ®çš„ç‰¹æ®Šéœ€æ±‚ã€‚

### æ ¸å¿ƒç‰¹ç‚¹

1. **æ··åˆç‰¹å¾å¤„ç†**ï¼šåŒæ—¶å¤„ç†æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾
2. **æ•°å€¼åµŒå…¥ï¼ˆPWLï¼‰**ï¼šä½¿ç”¨åˆ†æ®µçº¿æ€§åµŒå…¥å¤„ç†è¿ç»­æ•°å€¼
3. **Transformer æ¶æ„**ï¼šä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶å­¦ä¹ ç‰¹å¾äº¤äº’
4. **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šä»åŸå§‹ç‰¹å¾åˆ°é¢„æµ‹çš„å®Œæ•´æµç¨‹

---

## æ ¸å¿ƒæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
è¾“å…¥æ•°æ®
  â”‚
  â”œâ”€ æ•°å€¼ç‰¹å¾ â”€â”€> PWL æ•°å€¼åµŒå…¥ â”€â”€â”
  â”‚                              â”‚
  â””â”€ åˆ†ç±»ç‰¹å¾ â”€â”€> åˆ†ç±»åµŒå…¥ â”€â”€â”€â”€â”€â”€â”¤
                                 â”‚
                                 â–¼
                          ç‰¹å¾æ‹¼æ¥/èåˆ
                                 â”‚
                                 â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  TabM Block 1        â”‚
                    â”‚  - Self-Attention    â”‚
                    â”‚  - FFN               â”‚
                    â”‚  - Residual          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  TabM Block 2       â”‚
                    â”‚  ...                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  TabM Block N       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                           å…¨å±€æ± åŒ–/èšåˆ
                                 â”‚
                                 â–¼
                            è¾“å‡ºå±‚
                                 â”‚
                                 â–¼
                           å›å½’é¢„æµ‹å€¼
```

### æ¶æ„å±‚æ¬¡

1. **è¾“å…¥å±‚**ï¼šç‰¹å¾åµŒå…¥
2. **ç¼–ç å±‚**ï¼šå¤šä¸ª TabM Blockï¼ˆTransformer é£æ ¼ï¼‰
3. **èšåˆå±‚**ï¼šç‰¹å¾èšåˆ
4. **è¾“å‡ºå±‚**ï¼šå›å½’é¢„æµ‹

---

## å…³é”®ç»„ä»¶å®ç°

### 1. æ•°å€¼ç‰¹å¾åµŒå…¥ï¼ˆPWL - Piecewise Linearï¼‰

**åŸç†**ï¼šå°†è¿ç»­æ•°å€¼ç‰¹å¾åˆ†æˆå¤šä¸ªåŒºé—´ï¼ˆbinsï¼‰ï¼Œæ¯ä¸ªåŒºé—´å­¦ä¹ ä¸€ä¸ªåµŒå…¥å‘é‡ï¼Œé€šè¿‡çº¿æ€§æ’å€¼è®¡ç®—æœ€ç»ˆåµŒå…¥ã€‚

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class PWLNumericEmbedding(nn.Module):
    """
    åˆ†æ®µçº¿æ€§æ•°å€¼åµŒå…¥ï¼ˆPiecewise Linear Embeddingï¼‰
    ç±»ä¼¼ TabM ä¸­çš„æ•°å€¼ç‰¹å¾å¤„ç†æ–¹å¼
    """
    def __init__(self, num_features, n_bins=112, embed_dim=24):
        """
        Args:
            num_features: æ•°å€¼ç‰¹å¾æ•°é‡
            n_bins: åˆ†ç®±æ•°é‡ï¼ˆå¯¹åº” num_emb_n_binsï¼‰
            embed_dim: åµŒå…¥ç»´åº¦ï¼ˆå¯¹åº” d_embeddingï¼‰
        """
        super().__init__()
        self.num_features = num_features
        self.n_bins = n_bins
        self.embed_dim = embed_dim
        
        # ä¸ºæ¯ä¸ªæ•°å€¼ç‰¹å¾åˆ›å»ºåµŒå…¥è¡¨
        self.embeddings = nn.ModuleList([
            nn.Embedding(n_bins, embed_dim) for _ in range(num_features)
        ])
        
        # å¯å­¦ä¹ çš„è¾¹ç•Œç‚¹ï¼ˆbin edgesï¼‰
        # åˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
        self.bin_edges = nn.ParameterList([
            nn.Parameter(torch.linspace(0, 1, n_bins - 1)) 
            for _ in range(num_features)
        ])
        
        # å¯å­¦ä¹ çš„æƒé‡ï¼ˆç”¨äºæ’å€¼ï¼‰
        self.weights = nn.ParameterList([
            nn.Parameter(torch.ones(n_bins)) 
            for _ in range(num_features)
        ])
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features) æ•°å€¼ç‰¹å¾
        Returns:
            embedded: (batch_size, num_features, embed_dim) åµŒå…¥å‘é‡
        """
        batch_size, num_features = x.shape
        embedded_features = []
        
        for i in range(num_features):
            # 1. å½’ä¸€åŒ–åˆ° [0, 1]
            x_i = x[:, i]
            x_min = x_i.min()
            x_max = x_i.max()
            if x_max > x_min:
                x_norm = (x_i - x_min) / (x_max - x_min + 1e-8)
            else:
                x_norm = torch.zeros_like(x_i)
            
            # 2. æ‰¾åˆ°å¯¹åº”çš„ bin ç´¢å¼•
            # ä½¿ç”¨ bucketize æ‰¾åˆ°æ¯ä¸ªå€¼å±äºå“ªä¸ªåŒºé—´
            bin_indices = torch.bucketize(x_norm, self.bin_edges[i], right=True)
            bin_indices = torch.clamp(bin_indices, 0, self.n_bins - 1)
            
            # 3. è·å–åŸºç¡€åµŒå…¥
            base_embed = self.embeddings[i](bin_indices)  # (batch_size, embed_dim)
            
            # 4. çº¿æ€§æ’å€¼ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            # è®¡ç®—åœ¨ bin å†…çš„ä½ç½®
            bin_width = 1.0 / self.n_bins
            bin_pos = (x_norm - bin_indices.float() * bin_width) / bin_width
            bin_pos = torch.clamp(bin_pos, 0, 1)
            
            # 5. åº”ç”¨æƒé‡
            weight = self.weights[i][bin_indices].unsqueeze(-1)  # (batch_size, 1)
            embedded = base_embed * weight * (1 + bin_pos.unsqueeze(-1))
            
            embedded_features.append(embedded)
        
        # å †å æ‰€æœ‰ç‰¹å¾
        return torch.stack(embedded_features, dim=1)  # (batch_size, num_features, embed_dim)
```

### 2. åˆ†ç±»ç‰¹å¾åµŒå…¥

```python
class CategoricalEmbedding(nn.Module):
    """
    åˆ†ç±»ç‰¹å¾åµŒå…¥
    """
    def __init__(self, categorical_cardinalities, embed_dim=24):
        """
        Args:
            categorical_cardinalities: æ¯ä¸ªåˆ†ç±»ç‰¹å¾çš„åŸºæ•°ï¼ˆç±»åˆ«æ•°é‡ï¼‰åˆ—è¡¨
            embed_dim: åµŒå…¥ç»´åº¦
        """
        super().__init__()
        self.embeddings = nn.ModuleList([
            nn.Embedding(cardinality, embed_dim)
            for cardinality in categorical_cardinalities
        ])
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_categorical) åˆ†ç±»ç‰¹å¾ç´¢å¼•
        Returns:
            embedded: (batch_size, num_categorical, embed_dim) åµŒå…¥å‘é‡
        """
        embedded_features = []
        for i, emb in enumerate(self.embeddings):
            embedded_features.append(emb(x[:, i]))
        return torch.stack(embedded_features, dim=1)
```

### 3. TabM Blockï¼ˆTransformer é£æ ¼ï¼‰

```python
class TabMBlock(nn.Module):
    """
    TabM çš„æ ¸å¿ƒå—ï¼ŒåŸºäº Transformer æ¶æ„
    å¯¹åº”å‚æ•°ï¼šn_blocks, d_block
    """
    def __init__(self, d_model, n_heads=8, d_ff=None, dropout=0.0, tabm_k=32):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆå¯¹åº” d_blockï¼‰
            n_heads: æ³¨æ„åŠ›å¤´æ•°
            d_ff: å‰é¦ˆç½‘ç»œç»´åº¦ï¼ˆé€šå¸¸ä¸º d_model * 4ï¼‰
            dropout: Dropout æ¦‚ç‡
            tabm_k: TabM çš„ k å‚æ•°ï¼ˆæ§åˆ¶æ³¨æ„åŠ›èŒƒå›´ï¼‰
        """
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_ff = d_ff or d_model * 4
        self.tabm_k = tabm_k
        
        # è‡ªæ³¨æ„åŠ›å±‚
        self.self_attn = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        
        # å‰é¦ˆç½‘ç»œ
        self.ffn = nn.Sequential(
            nn.Linear(d_model, self.d_ff),
            nn.GELU(),  # æˆ– ReLU
            nn.Dropout(dropout),
            nn.Linear(self.d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # TabM ç‰¹å®šçš„ç‰¹å¾é€‰æ‹©æœºåˆ¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
        self.feature_selector = nn.Linear(d_model, tabm_k)
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features, d_model) ç‰¹å¾åµŒå…¥
        Returns:
            out: (batch_size, num_features, d_model) è¾“å‡º
        """
        # 1. è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        residual = x
        x = self.norm1(x)
        
        # TabM ç‰¹å®šçš„æ³¨æ„åŠ›æœºåˆ¶
        # å¯ä»¥é€‰æ‹© top-k ç‰¹å¾è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—
        attn_weights = self.feature_selector(x)  # (batch_size, num_features, k)
        attn_weights = F.softmax(attn_weights, dim=-1)
        
        # ç®€åŒ–çš„æ³¨æ„åŠ›è®¡ç®—
        attn_out, _ = self.self_attn(x, x, x)
        x = residual + attn_out
        
        # 2. å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿æ¥
        residual = x
        x = self.norm2(x)
        x = residual + self.ffn(x)
        
        return x
```

### 4. ç‰¹å¾èåˆå±‚

```python
class FeatureFusion(nn.Module):
    """
    èåˆæ•°å€¼å’Œåˆ†ç±»ç‰¹å¾
    """
    def __init__(self, num_numeric, num_categorical, embed_dim=24):
        super().__init__()
        self.num_numeric = num_numeric
        self.num_categorical = num_categorical
        self.embed_dim = embed_dim
        
        # ç‰¹å¾ç±»å‹åµŒå…¥ï¼ˆå¯é€‰ï¼‰
        self.type_embedding = nn.Embedding(2, embed_dim)  # 0: numeric, 1: categorical
    
    def forward(self, numeric_emb, categorical_emb):
        """
        Args:
            numeric_emb: (batch_size, num_numeric, embed_dim)
            categorical_emb: (batch_size, num_categorical, embed_dim)
        Returns:
            fused: (batch_size, num_features, embed_dim)
        """
        # æ·»åŠ ç±»å‹åµŒå…¥
        numeric_type = self.type_embedding(torch.zeros(self.num_numeric, dtype=torch.long))
        categorical_type = self.type_embedding(torch.ones(self.num_categorical, dtype=torch.long))
        
        numeric_emb = numeric_emb + numeric_type.unsqueeze(0)
        categorical_emb = categorical_emb + categorical_type.unsqueeze(0)
        
        # æ‹¼æ¥
        fused = torch.cat([numeric_emb, categorical_emb], dim=1)
        return fused
```

---

## å®Œæ•´å®ç°ä»£ç 

### TabM_D_Regressor å®Œæ•´å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from typing import List, Optional

class TabM_D_Regressor:
    """
    TabM (Tabular Model) å›å½’å™¨
    åŸºäº Transformer æ¶æ„çš„è¡¨æ ¼æ•°æ®æ·±åº¦å­¦ä¹ æ¨¡å‹
    """
    
    def __init__(
        self,
        batch_size='auto',
        patience=16,
        allow_amp=False,
        arch_type='tabm-mini',
        tabm_k=32,
        gradient_clipping_norm=1.0,
        share_training_batches=False,
        lr=0.000624068703424289,
        weight_decay=0.0019090968357478807,
        n_blocks=5,
        d_block=432,
        dropout=0.0,
        num_emb_type='pwl',
        d_embedding=24,
        num_emb_n_bins=112,
        device='cuda' if torch.cuda.is_available() else 'cpu',
    ):
        """
        åˆå§‹åŒ– TabM æ¨¡å‹
        
        Args:
            batch_size: æ‰¹æ¬¡å¤§å°ï¼Œ'auto' è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—
            patience: æ—©åœè€å¿ƒå€¼
            allow_amp: æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            arch_type: æ¶æ„ç±»å‹ï¼ˆ'tabm-mini', 'tabm-base', 'tabm-large'ï¼‰
            tabm_k: TabM çš„ k å‚æ•°
            gradient_clipping_norm: æ¢¯åº¦è£å‰ªèŒƒæ•°
            lr: å­¦ä¹ ç‡
            weight_decay: æƒé‡è¡°å‡
            n_blocks: TabM Block æ•°é‡
            d_block: æ¯ä¸ª Block çš„ç»´åº¦
            dropout: Dropout æ¦‚ç‡
            num_emb_type: æ•°å€¼åµŒå…¥ç±»å‹ï¼ˆ'pwl' è¡¨ç¤ºåˆ†æ®µçº¿æ€§ï¼‰
            d_embedding: åµŒå…¥ç»´åº¦
            num_emb_n_bins: æ•°å€¼åµŒå…¥åˆ†ç®±æ•°
            device: è®¡ç®—è®¾å¤‡
        """
        self.patience = patience
        self.allow_amp = allow_amp
        self.arch_type = arch_type
        self.tabm_k = tabm_k
        self.gradient_clipping_norm = gradient_clipping_norm
        self.lr = lr
        self.weight_decay = weight_decay
        self.n_blocks = n_blocks
        self.d_block = d_block
        self.dropout = dropout
        self.num_emb_type = num_emb_type
        self.d_embedding = d_embedding
        self.num_emb_n_bins = num_emb_n_bins
        self.device = device
        
        # è‡ªåŠ¨è®¡ç®—æ‰¹æ¬¡å¤§å°
        if batch_size == 'auto':
            self.batch_size = 256 if device == 'cuda' else 32
        else:
            self.batch_size = batch_size
        
        # æ¨¡å‹ç»„ä»¶ï¼ˆå°†åœ¨ fit æ—¶åˆå§‹åŒ–ï¼‰
        self.model = None
        self.numeric_scaler = StandardScaler()
        self.categorical_encoders = []
        self.num_numeric_features = 0
        self.num_categorical_features = 0
        self.categorical_cardinalities = []
    
    def _build_model(self, num_numeric, categorical_cardinalities):
        """æ„å»ºæ¨¡å‹æ¶æ„"""
        return TabMNet(
            num_numeric=num_numeric,
            categorical_cardinalities=categorical_cardinalities,
            d_embedding=self.d_embedding,
            num_emb_n_bins=self.num_emb_n_bins,
            n_blocks=self.n_blocks,
            d_block=self.d_block,
            dropout=self.dropout,
            tabm_k=self.tabm_k,
        ).to(self.device)
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, cat_col_names=None):
        """
        è®­ç»ƒæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾ï¼ˆDataFrameï¼‰
            y_train: è®­ç»ƒç›®æ ‡ï¼ˆSeriesï¼‰
            X_val: éªŒè¯ç‰¹å¾ï¼ˆDataFrameï¼Œå¯é€‰ï¼‰
            y_val: éªŒè¯ç›®æ ‡ï¼ˆSeriesï¼Œå¯é€‰ï¼‰
            cat_col_names: åˆ†ç±»ç‰¹å¾åˆ—ååˆ—è¡¨
        """
        # 1. æ•°æ®é¢„å¤„ç†
        X_train_processed, X_val_processed = self._preprocess_data(
            X_train, X_val, cat_col_names, fit=True
        )
        
        y_train_tensor = torch.FloatTensor(y_train.values).to(self.device)
        y_val_tensor = torch.FloatTensor(y_val.values).to(self.device) if y_val is not None else None
        
        # 2. æ„å»ºæ¨¡å‹
        self.model = self._build_model(
            self.num_numeric_features,
            self.categorical_cardinalities
        )
        
        # 3. è®­ç»ƒ
        self._train(
            X_train_processed, y_train_tensor,
            X_val_processed, y_val_tensor
        )
    
    def _preprocess_data(self, X_train, X_val=None, cat_col_names=None, fit=False):
        """æ•°æ®é¢„å¤„ç†"""
        if cat_col_names is None:
            cat_col_names = []
        
        # åˆ†ç¦»æ•°å€¼å’Œåˆ†ç±»ç‰¹å¾
        numeric_cols = [col for col in X_train.columns if col not in cat_col_names]
        categorical_cols = cat_col_names
        
        # å¤„ç†æ•°å€¼ç‰¹å¾
        X_train_numeric = X_train[numeric_cols].values
        if fit:
            X_train_numeric = self.numeric_scaler.fit_transform(X_train_numeric)
            self.num_numeric_features = len(numeric_cols)
        else:
            X_train_numeric = self.numeric_scaler.transform(X_train_numeric)
        
        # å¤„ç†åˆ†ç±»ç‰¹å¾
        X_train_categorical = []
        if fit:
            self.categorical_encoders = []
            self.categorical_cardinalities = []
        
        for i, col in enumerate(categorical_cols):
            if fit:
                le = LabelEncoder()
                X_train_cat = le.fit_transform(X_train[col].astype(str).fillna('unknown'))
                self.categorical_encoders.append(le)
                self.categorical_cardinalities.append(len(le.classes_))
            else:
                le = self.categorical_encoders[i]
                X_train_cat = le.transform(X_train[col].astype(str).fillna('unknown'))
            X_train_categorical.append(X_train_cat)
        
        X_train_categorical = np.column_stack(X_train_categorical) if X_train_categorical else np.array([]).reshape(len(X_train), 0)
        
        # å¤„ç†éªŒè¯é›†
        if X_val is not None:
            X_val_numeric = self.numeric_scaler.transform(X_val[numeric_cols].values)
            X_val_categorical = []
            for i, col in enumerate(categorical_cols):
                le = self.categorical_encoders[i]
                X_val_cat = le.transform(X_val[col].astype(str).fillna('unknown'))
                X_val_categorical.append(X_val_cat)
            X_val_categorical = np.column_stack(X_val_categorical) if X_val_categorical else np.array([]).reshape(len(X_val), 0)
            return (X_train_numeric, X_train_categorical), (X_val_numeric, X_val_categorical)
        
        return (X_train_numeric, X_train_categorical), None
    
    def _train(self, X_train, y_train, X_val=None, y_val=None):
        """è®­ç»ƒå¾ªç¯"""
        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.lr,
            weight_decay=self.weight_decay
        )
        
        criterion = nn.MSELoss()
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=self.patience // 2, factor=0.5
        )
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TabularDataset(X_train, y_train)
        train_loader = torch.utils.data.DataLoader(
            train_dataset, batch_size=self.batch_size, shuffle=True
        )
        
        if X_val is not None:
            val_dataset = TabularDataset(X_val, y_val)
            val_loader = torch.utils.data.DataLoader(
                val_dataset, batch_size=self.batch_size, shuffle=False
            )
        
        for epoch in range(1000):  # æœ€å¤§ epoch æ•°
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0
            for batch in train_loader:
                numeric, categorical, target = batch
                numeric = numeric.to(self.device)
                categorical = categorical.to(self.device)
                target = target.to(self.device)
                
                optimizer.zero_grad()
                
                if self.allow_amp:
                    with torch.cuda.amp.autocast():
                        pred = self.model(numeric, categorical)
                        loss = criterion(pred, target)
                    torch.cuda.amp.scale_loss(loss, optimizer).backward()
                else:
                    pred = self.model(numeric, categorical)
                    loss = criterion(pred, target)
                    loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), self.gradient_clipping_norm
                )
                
                optimizer.step()
                train_loss += loss.item()
            
            train_loss /= len(train_loader)
            
            # éªŒè¯é˜¶æ®µ
            if X_val is not None:
                self.model.eval()
                val_loss = 0
                with torch.no_grad():
                    for batch in val_loader:
                        numeric, categorical, target = batch
                        numeric = numeric.to(self.device)
                        categorical = categorical.to(self.device)
                        target = target.to(self.device)
                        
                        pred = self.model(numeric, categorical)
                        loss = criterion(pred, target)
                        val_loss += loss.item()
                
                val_loss /= len(val_loader)
                scheduler.step(val_loss)
                
                # æ—©åœæ£€æŸ¥
                if val_loss < best_val_loss:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    self.best_model_state = self.model.state_dict().copy()
                else:
                    patience_counter += 1
                
                if patience_counter >= self.patience:
                    print(f'Early stopping at epoch {epoch+1}')
                    self.model.load_state_dict(self.best_model_state)
                    break
                
                if (epoch + 1) % 10 == 0:
                    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.5f}, Val Loss: {val_loss:.5f}')
            else:
                if (epoch + 1) % 10 == 0:
                    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.5f}')
    
    def predict(self, X):
        """é¢„æµ‹"""
        X_processed, _ = self._preprocess_data(X, fit=False)
        
        self.model.eval()
        predictions = []
        
        dataset = TabularDataset(X_processed, None)
        loader = torch.utils.data.DataLoader(
            dataset, batch_size=self.batch_size, shuffle=False
        )
        
        with torch.no_grad():
            for batch in loader:
                numeric, categorical = batch
                numeric = numeric.to(self.device)
                categorical = categorical.to(self.device)
                
                pred = self.model(numeric, categorical)
                predictions.append(pred.cpu().numpy())
        
        return np.concatenate(predictions)


class TabMNet(nn.Module):
    """
    TabM ç¥ç»ç½‘ç»œæ¶æ„
    """
    def __init__(
        self,
        num_numeric,
        categorical_cardinalities,
        d_embedding=24,
        num_emb_n_bins=112,
        n_blocks=5,
        d_block=432,
        dropout=0.0,
        tabm_k=32,
    ):
        super().__init__()
        
        # 1. ç‰¹å¾åµŒå…¥å±‚
        if num_numeric > 0:
            self.numeric_embedding = PWLNumericEmbedding(
                num_numeric, num_emb_n_bins, d_embedding
            )
        else:
            self.numeric_embedding = None
        
        if len(categorical_cardinalities) > 0:
            self.categorical_embedding = CategoricalEmbedding(
                categorical_cardinalities, d_embedding
            )
        else:
            self.categorical_embedding = None
        
        # 2. ç‰¹å¾èåˆ
        self.feature_fusion = FeatureFusion(
            num_numeric, len(categorical_cardinalities), d_embedding
        )
        
        # 3. æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        num_features = num_numeric + len(categorical_cardinalities)
        self.input_projection = nn.Linear(d_embedding, d_block)
        
        # 4. TabM Blocks
        self.blocks = nn.ModuleList([
            TabMBlock(d_block, dropout=dropout, tabm_k=tabm_k)
            for _ in range(n_blocks)
        ])
        
        # 5. å…¨å±€èšåˆ
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # 6. è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(d_block, d_block // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_block // 2, 1)
        )
    
    def forward(self, numeric_features, categorical_features):
        """
        Args:
            numeric_features: (batch_size, num_numeric) æˆ– None
            categorical_features: (batch_size, num_categorical) æˆ– None
        Returns:
            output: (batch_size, 1) é¢„æµ‹å€¼
        """
        # 1. ç‰¹å¾åµŒå…¥
        embedded_features = []
        
        if self.numeric_embedding is not None:
            numeric_emb = self.numeric_embedding(numeric_features)
            embedded_features.append(numeric_emb)
        
        if self.categorical_embedding is not None:
            categorical_emb = self.categorical_embedding(categorical_features)
            embedded_features.append(categorical_emb)
        
        # 2. ç‰¹å¾èåˆ
        if len(embedded_features) == 2:
            x = self.feature_fusion(embedded_features[0], embedded_features[1])
        elif len(embedded_features) == 1:
            x = embedded_features[0]
        else:
            raise ValueError("è‡³å°‘éœ€è¦ä¸€ç§ç‰¹å¾ç±»å‹")
        
        # 3. æŠ•å½±åˆ°æ¨¡å‹ç»´åº¦
        x = self.input_projection(x)  # (batch_size, num_features, d_block)
        
        # 4. TabM Blocks
        for block in self.blocks:
            x = block(x)
        
        # 5. å…¨å±€èšåˆï¼ˆå¹³å‡æ± åŒ–ï¼‰
        x = x.transpose(1, 2)  # (batch_size, d_block, num_features)
        x = self.global_pool(x).squeeze(-1)  # (batch_size, d_block)
        
        # 6. è¾“å‡º
        output = self.output_layer(x)  # (batch_size, 1)
        
        return output.squeeze(-1)


class TabularDataset(torch.utils.data.Dataset):
    """è¡¨æ ¼æ•°æ®æ•°æ®é›†"""
    def __init__(self, features, targets=None):
        self.numeric = torch.FloatTensor(features[0])
        self.categorical = torch.LongTensor(features[1]) if features[1].size > 0 else None
        self.targets = torch.FloatTensor(targets) if targets is not None else None
    
    def __len__(self):
        return len(self.numeric)
    
    def __getitem__(self, idx):
        if self.targets is not None:
            if self.categorical is not None:
                return self.numeric[idx], self.categorical[idx], self.targets[idx]
            else:
                return self.numeric[idx], torch.tensor([]), self.targets[idx]
        else:
            if self.categorical is not None:
                return self.numeric[idx], self.categorical[idx]
            else:
                return self.numeric[idx], torch.tensor([])
```

---

## è®­ç»ƒæµç¨‹åˆ†æ

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
# 1. åˆå§‹åŒ–æ¨¡å‹
model = TabM_D_Regressor(
    arch_type='tabm-mini',
    tabm_k=32,
    n_blocks=5,
    d_block=432,
    lr=0.000624,
    weight_decay=0.001909,
    d_embedding=24,
    num_emb_n_bins=112,
)

# 2. è®­ç»ƒ
model.fit(
    X_train, y_train,
    X_val, y_val,
    cat_col_names=['road_type', 'lighting', 'weather', ...]
)

# 3. é¢„æµ‹
predictions = model.predict(X_test)
```

### æ•°æ®æµ

```
åŸå§‹æ•°æ® (DataFrame)
  â”‚
  â”œâ”€ æ•°å€¼ç‰¹å¾ â”€â”€> StandardScaler â”€â”€> PWL åµŒå…¥ â”€â”€â”
  â”‚                                              â”‚
  â””â”€ åˆ†ç±»ç‰¹å¾ â”€â”€> LabelEncoder â”€â”€> åˆ†ç±»åµŒå…¥ â”€â”€â”€â”€â”€â”¤
                                                 â”‚
                                                 â–¼
                                          ç‰¹å¾èåˆ
                                                 â”‚
                                                 â–¼
                                         è¾“å…¥æŠ•å½± (d_embedding -> d_block)
                                                 â”‚
                                                 â–¼
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   TabM Block 1          â”‚
                                    â”‚   - Self-Attention      â”‚
                                    â”‚   - FFN                 â”‚
                                    â”‚   - Residual            â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â”‚   TabM Block 2-5        â”‚
                                    â”‚   ...                   â”‚
                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                                                 â–¼
                                          å…¨å±€å¹³å‡æ± åŒ–
                                                 â”‚
                                                 â–¼
                                            è¾“å‡ºå±‚
                                                 â”‚
                                                 â–¼
                                           é¢„æµ‹å€¼
```

---

## å…³é”®æŠ€æœ¯è¯¦è§£

### 1. PWL æ•°å€¼åµŒå…¥è¯¦è§£

**ä¸ºä»€ä¹ˆä½¿ç”¨ PWLï¼Ÿ**

1. **éçº¿æ€§æ˜ å°„**ï¼šå°†è¿ç»­æ•°å€¼æ˜ å°„åˆ°åµŒå…¥ç©ºé—´ï¼Œæ•æ‰éçº¿æ€§å…³ç³»
2. **åˆ†ç®±ç­–ç•¥**ï¼šå°†è¿ç»­å€¼åˆ†æˆå¤šä¸ªåŒºé—´ï¼Œæ¯ä¸ªåŒºé—´å­¦ä¹ ä¸åŒçš„è¡¨ç¤º
3. **å¯å­¦ä¹ è¾¹ç•Œ**ï¼šbin edges æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œå¯ä»¥è‡ªé€‚åº”è°ƒæ•´

**å®ç°ç»†èŠ‚**ï¼š

```python
# ç¤ºä¾‹ï¼šå¤„ç†ä¸€ä¸ªæ•°å€¼ç‰¹å¾
x = 0.75  # å½’ä¸€åŒ–åçš„å€¼
n_bins = 112

# 1. æ‰¾åˆ°å¯¹åº”çš„ bin
bin_idx = int(x * n_bins)  # 84

# 2. è·å–è¯¥ bin çš„åµŒå…¥å‘é‡
embedding = embedding_table[bin_idx]  # (embed_dim,)

# 3. çº¿æ€§æ’å€¼ï¼ˆå¯é€‰ï¼‰
bin_pos = (x - bin_idx / n_bins) * n_bins  # åœ¨ bin å†…çš„ä½ç½®
# å¯ä»¥æ’å€¼ç›¸é‚» bin çš„åµŒå…¥
```

### 2. TabM Block è¯¦è§£

**æ ¸å¿ƒç»„ä»¶**ï¼š

1. **Self-Attention**ï¼šå­¦ä¹ ç‰¹å¾é—´çš„äº¤äº’å…³ç³»
2. **FFN (Feed-Forward Network)**ï¼šéçº¿æ€§å˜æ¢
3. **Residual Connection**ï¼šç¼“è§£æ¢¯åº¦æ¶ˆå¤±
4. **Layer Normalization**ï¼šç¨³å®šè®­ç»ƒ

**æ³¨æ„åŠ›æœºåˆ¶**ï¼š

```python
# è‡ªæ³¨æ„åŠ›è®¡ç®—
Q = Linear(x)  # Query
K = Linear(x)  # Key
V = Linear(x)  # Value

# æ³¨æ„åŠ›åˆ†æ•°
scores = Q @ K.T / sqrt(d_k)
attn_weights = softmax(scores)

# åŠ æƒæ±‚å’Œ
output = attn_weights @ V
```

### 3. ç‰¹å¾äº¤äº’å­¦ä¹ 

**TabM å¦‚ä½•å­¦ä¹ ç‰¹å¾äº¤äº’ï¼Ÿ**

1. **æ³¨æ„åŠ›æƒé‡**ï¼šæ˜¾ç¤ºå“ªäº›ç‰¹å¾å¯¹é¢„æµ‹æœ€é‡è¦
2. **å¤šå±‚å †å **ï¼šæ¯å±‚å­¦ä¹ ä¸åŒå±‚æ¬¡çš„äº¤äº’
3. **å…¨å±€èšåˆ**ï¼šå°†æ‰€æœ‰ç‰¹å¾ä¿¡æ¯èåˆ

**ç¤ºä¾‹**ï¼š

```
Block 1: å­¦ä¹ åŸºç¡€ç‰¹å¾è¡¨ç¤º
Block 2: å­¦ä¹ ä¸¤ä¸¤ç‰¹å¾äº¤äº’
Block 3: å­¦ä¹ é«˜é˜¶ç‰¹å¾äº¤äº’
Block 4-5: è¿›ä¸€æ­¥ç²¾ç‚¼è¡¨ç¤º
```

### 4. è®­ç»ƒæŠ€å·§

#### 4.1 æ¢¯åº¦è£å‰ª

```python
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**ä½œç”¨**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ï¼Œç¨³å®šè®­ç»ƒ

#### 4.2 å­¦ä¹ ç‡è°ƒåº¦

```python
scheduler = ReduceLROnPlateau(optimizer, patience=8, factor=0.5)
```

**ä½œç”¨**ï¼šéªŒè¯æŸå¤±ä¸ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡

#### 4.3 æ—©åœæœºåˆ¶

```python
if val_loss < best_val_loss:
    best_val_loss = val_loss
    patience_counter = 0
    save_best_model()
else:
    patience_counter += 1
    if patience_counter >= patience:
        stop_training()
```

**ä½œç”¨**ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼ŒèŠ‚çœè®­ç»ƒæ—¶é—´

---

## å‚æ•°è¯´æ˜

### å…³é”®è¶…å‚æ•°

| å‚æ•° | è¯´æ˜ | å…¸å‹å€¼ | å½±å“ |
|------|------|--------|------|
| `n_blocks` | TabM Block æ•°é‡ | 5 | æ¨¡å‹æ·±åº¦ï¼Œè¶Šå¤šè¶Šå¤æ‚ |
| `d_block` | Block ç»´åº¦ | 432 | æ¨¡å‹å®¹é‡ï¼Œè¶Šå¤§è¡¨è¾¾èƒ½åŠ›è¶Šå¼º |
| `d_embedding` | åµŒå…¥ç»´åº¦ | 24 | ç‰¹å¾è¡¨ç¤ºç»´åº¦ |
| `num_emb_n_bins` | æ•°å€¼åµŒå…¥åˆ†ç®±æ•° | 112 | æ•°å€¼ç‰¹å¾ç¦»æ•£åŒ–ç²’åº¦ |
| `tabm_k` | TabM k å‚æ•° | 32 | æ³¨æ„åŠ›èŒƒå›´æ§åˆ¶ |
| `lr` | å­¦ä¹ ç‡ | 0.000624 | è®­ç»ƒé€Ÿåº¦ï¼Œå¤ªå¤§å¯èƒ½ä¸ç¨³å®š |
| `weight_decay` | æƒé‡è¡°å‡ | 0.001909 | æ­£åˆ™åŒ–å¼ºåº¦ |
| `dropout` | Dropout æ¦‚ç‡ | 0.0 | é˜²æ­¢è¿‡æ‹Ÿåˆ |
| `patience` | æ—©åœè€å¿ƒå€¼ | 16 | æ—©åœç­‰å¾…è½®æ•° |

### å‚æ•°è°ƒä¼˜å»ºè®®

1. **ä»å°å¼€å§‹**ï¼šå…ˆä½¿ç”¨è¾ƒå°çš„ `n_blocks` å’Œ `d_block`
2. **é€æ­¥å¢åŠ **ï¼šå¦‚æœæ¬ æ‹Ÿåˆï¼Œå¢åŠ æ¨¡å‹å®¹é‡
3. **å­¦ä¹ ç‡**ï¼šé€šå¸¸ä» 1e-4 åˆ° 1e-3 ä¹‹é—´
4. **æ‰¹æ¬¡å¤§å°**ï¼šGPU å†…å­˜å…è®¸çš„æƒ…å†µä¸‹ï¼Œè¶Šå¤§è¶Šå¥½
5. **æ—©åœ**ï¼šæ ¹æ®éªŒè¯é›†è¡¨ç°è°ƒæ•´ `patience`

---

## æ€§èƒ½ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    pred = model(x)
    loss = criterion(pred, y)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

**ä¼˜åŠ¿**ï¼šå‡å°‘æ˜¾å­˜å ç”¨ï¼ŒåŠ é€Ÿè®­ç»ƒ

### 2. æ•°æ®å¹¶è¡Œ

```python
if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model)
```

**ä¼˜åŠ¿**ï¼šå¤š GPU åŠ é€Ÿè®­ç»ƒ

### 3. æ‰¹æ¬¡å¤§å°ä¼˜åŒ–

```python
# è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°
if device == 'cuda':
    batch_size = 256  # GPU å¯ä»¥æ›´å¤§
else:
    batch_size = 32   # CPU è¾ƒå°
```

---

## æ€»ç»“

### TabM çš„æ ¸å¿ƒä¼˜åŠ¿

1. **ä¸“é—¨è®¾è®¡**ï¼šé’ˆå¯¹è¡¨æ ¼æ•°æ®ä¼˜åŒ–
2. **æ··åˆç‰¹å¾**ï¼šåŸç”Ÿæ”¯æŒæ•°å€¼å’Œåˆ†ç±»ç‰¹å¾
3. **ç‰¹å¾äº¤äº’**ï¼šè‡ªåŠ¨å­¦ä¹ å¤æ‚äº¤äº’å…³ç³»
4. **å¯æ‰©å±•**ï¼šå¯ä»¥é€šè¿‡å¢åŠ  blocks æé«˜æ€§èƒ½

### å®ç°è¦ç‚¹

1. **PWL åµŒå…¥**ï¼šå¤„ç†æ•°å€¼ç‰¹å¾çš„å…³é”®
2. **Transformer æ¶æ„**ï¼šå­¦ä¹ ç‰¹å¾äº¤äº’
3. **ç«¯åˆ°ç«¯è®­ç»ƒ**ï¼šä»åŸå§‹ç‰¹å¾åˆ°é¢„æµ‹
4. **è®­ç»ƒæŠ€å·§**ï¼šæ¢¯åº¦è£å‰ªã€å­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœ

### é€‚ç”¨åœºæ™¯

- âœ… è¡¨æ ¼æ•°æ®å›å½’/åˆ†ç±»ä»»åŠ¡
- âœ… ç‰¹å¾äº¤äº’å¤æ‚
- âœ… æœ‰ GPU èµ„æº
- âœ… è¿½æ±‚é«˜æ€§èƒ½

### å±€é™æ€§

- âŒ è®­ç»ƒæ—¶é—´é•¿
- âŒ éœ€è¦ GPUï¼ˆCPU è®­ç»ƒå¾ˆæ…¢ï¼‰
- âŒ éœ€è¦è°ƒå‚
- âŒ å†…å­˜å ç”¨è¾ƒå¤§

---

## å‚è€ƒèµ„æ–™

- **pytabkit**: https://github.com/georgian-io/pytabkit
- **Transformer è®ºæ–‡**: "Attention Is All You Need"
- **TabM ç›¸å…³ç ”ç©¶**: è¡¨æ ¼æ•°æ®æ·±åº¦å­¦ä¹ æ–¹æ³•

