# FFN (Feed-Forward Network) åŠç›¸å…³çŸ¥è¯†è¯¦è§£

## ğŸ“‹ ç›®å½•

1. [FFN åŸºç¡€æ¦‚å¿µ](#ffn-åŸºç¡€æ¦‚å¿µ)
2. [FFN åœ¨ Transformer ä¸­çš„ä½œç”¨](#ffn-åœ¨-transformer-ä¸­çš„ä½œç”¨)
3. [FFN å®ç°è¯¦è§£](#ffn-å®ç°è¯¦è§£)
4. [æ¿€æ´»å‡½æ•°](#æ¿€æ´»å‡½æ•°)
5. [æ­£åˆ™åŒ–æŠ€æœ¯](#æ­£åˆ™åŒ–æŠ€æœ¯)
6. [å…¶ä»–ç›¸å…³æ¦‚å¿µ](#å…¶ä»–ç›¸å…³æ¦‚å¿µ)
7. [å®Œæ•´ä»£ç ç¤ºä¾‹](#å®Œæ•´ä»£ç ç¤ºä¾‹)

---

## FFN åŸºç¡€æ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ FFNï¼Ÿ

**FFN (Feed-Forward Network)**ï¼Œä¹Ÿç§°ä¸º**å‰é¦ˆç¥ç»ç½‘ç»œ**æˆ–**å¤šå±‚æ„ŸçŸ¥æœº (MLP)**ï¼Œæ˜¯æ·±åº¦å­¦ä¹ ä¸­æœ€åŸºç¡€çš„ç½‘ç»œç»“æ„ã€‚

### åŸºæœ¬ç»“æ„

```
è¾“å…¥å±‚ (Input Layer)
    â†“
éšè—å±‚ 1 (Hidden Layer 1)
    â†“
éšè—å±‚ 2 (Hidden Layer 2)
    â†“
...
    â†“
è¾“å‡ºå±‚ (Output Layer)
```

### æ•°å­¦è¡¨ç¤º

å¯¹äºä¸€ä¸ªç®€å•çš„ FFNï¼š

```
y = f(Wâ‚‚ Â· f(Wâ‚ Â· x + bâ‚) + bâ‚‚)
```

å…¶ä¸­ï¼š
- `x`: è¾“å…¥å‘é‡
- `Wâ‚, Wâ‚‚`: æƒé‡çŸ©é˜µ
- `bâ‚, bâ‚‚`: åç½®å‘é‡
- `f`: æ¿€æ´»å‡½æ•°

### FFN çš„ç‰¹ç‚¹

1. **å•å‘ä¼ æ’­**ï¼šä¿¡æ¯ä»è¾“å…¥åˆ°è¾“å‡ºå•å‘æµåŠ¨
2. **å…¨è¿æ¥**ï¼šæ¯ä¸€å±‚çš„æ¯ä¸ªç¥ç»å…ƒéƒ½ä¸ä¸‹ä¸€å±‚çš„æ‰€æœ‰ç¥ç»å…ƒè¿æ¥
3. **éçº¿æ€§å˜æ¢**ï¼šé€šè¿‡æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§

---

## FFN åœ¨ Transformer ä¸­çš„ä½œç”¨

### Transformer æ¶æ„ä¸­çš„ FFN

åœ¨ Transformer æ¶æ„ä¸­ï¼ˆåŒ…æ‹¬ TabMï¼‰ï¼ŒFFN æ˜¯æ¯ä¸ª Block çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼š

```
Transformer Block:
    â”œâ”€ Multi-Head Self-Attention
    â”œâ”€ Add & Norm (æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–)
    â”œâ”€ FFN (Feed-Forward Network)  â† è¿™é‡Œ
    â””â”€ Add & Norm (æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–)
```

### FFN çš„ä½œç”¨

1. **éçº¿æ€§å˜æ¢**ï¼šæ³¨æ„åŠ›æœºåˆ¶æ˜¯çº¿æ€§å˜æ¢ï¼ŒFFN æä¾›éçº¿æ€§
2. **ç‰¹å¾å¢å¼º**ï¼šå°†æ³¨æ„åŠ›åçš„ç‰¹å¾è¿›ä¸€æ­¥å¤„ç†
3. **ç»´åº¦æ‰©å±•**ï¼šé€šå¸¸å…ˆæ‰©å±•åˆ°æ›´å¤§ç»´åº¦ï¼Œå†å‹ç¼©å›åŸç»´åº¦

### å…¸å‹çš„ FFN ç»“æ„

```python
FFN(x) = ReLU(Wâ‚‚ Â· ReLU(Wâ‚ Â· x + bâ‚) + bâ‚‚)
```

æˆ–è€…ä½¿ç”¨ GELUï¼š

```python
FFN(x) = GELU(Wâ‚‚ Â· GELU(Wâ‚ Â· x + bâ‚) + bâ‚‚)
```

**ç»´åº¦å˜åŒ–**ï¼š
- è¾“å…¥ï¼š`d_model` (ä¾‹å¦‚ 432)
- ä¸­é—´å±‚ï¼š`d_ff` (é€šå¸¸æ˜¯ `d_model * 4`ï¼Œä¾‹å¦‚ 1728)
- è¾“å‡ºï¼š`d_model` (ä¾‹å¦‚ 432)

---

## FFN å®ç°è¯¦è§£

### 1. åŸºç¡€ FFN å®ç°

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class BasicFFN(nn.Module):
    """åŸºç¡€çš„å‰é¦ˆç¥ç»ç½‘ç»œ"""
    def __init__(self, d_model, d_ff, activation='relu', dropout=0.0):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆè¾“å…¥å’Œè¾“å‡ºç»´åº¦ï¼‰
            d_ff: å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦ï¼ˆé€šå¸¸æ˜¯ d_model * 4ï¼‰
            activation: æ¿€æ´»å‡½æ•°ç±»å‹
            dropout: Dropout æ¦‚ç‡
        """
        super().__init__()
        
        # ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ° d_ff
        self.linear1 = nn.Linear(d_model, d_ff)
        
        # ç¬¬äºŒå±‚ï¼šå‹ç¼©å› d_model
        self.linear2 = nn.Linear(d_ff, d_model)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # æ¿€æ´»å‡½æ•°
        if activation == 'relu':
            self.activation = nn.ReLU()
        elif activation == 'gelu':
            self.activation = nn.GELU()
        elif activation == 'swish':
            self.activation = nn.SiLU()  # Swish = SiLU
        else:
            raise ValueError(f"Unknown activation: {activation}")
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model) æˆ– (batch_size, d_model)
        Returns:
            out: ç›¸åŒå½¢çŠ¶çš„è¾“å‡º
        """
        # ç¬¬ä¸€å±‚ï¼šæ‰©å±•ç»´åº¦
        x = self.linear1(x)  # (..., d_ff)
        x = self.activation(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚ï¼šå‹ç¼©å›åŸç»´åº¦
        x = self.linear2(x)  # (..., d_model)
        x = self.dropout(x)
        
        return x
```

### 2. å¸¦æ®‹å·®è¿æ¥çš„ FFN

```python
class FFNWithResidual(nn.Module):
    """å¸¦æ®‹å·®è¿æ¥çš„ FFN"""
    def __init__(self, d_model, d_ff, activation='gelu', dropout=0.0):
        super().__init__()
        self.ffn = BasicFFN(d_model, d_ff, activation, dropout)
        self.norm = nn.LayerNorm(d_model)
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, seq_len, d_model)
        Returns:
            out: (batch_size, seq_len, d_model)
        """
        # æ®‹å·®è¿æ¥
        residual = x
        x = self.norm(x)
        x = self.ffn(x)
        x = x + residual  # æ®‹å·®è¿æ¥
        return x
```

### 3. TabM ä¸­çš„ FFN å®ç°

```python
class TabMFFN(nn.Module):
    """TabM ä¸­ä½¿ç”¨çš„å‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model, d_ff=None, dropout=0.0):
        """
        Args:
            d_model: æ¨¡å‹ç»´åº¦ï¼ˆå¯¹åº” d_blockï¼Œä¾‹å¦‚ 432ï¼‰
            d_ff: å‰é¦ˆç½‘ç»œç»´åº¦ï¼ˆé€šå¸¸ä¸º d_model * 4ï¼‰
            dropout: Dropout æ¦‚ç‡
        """
        super().__init__()
        self.d_model = d_model
        self.d_ff = d_ff or (d_model * 4)  # é»˜è®¤æ‰©å±• 4 å€
        
        # ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ° d_ff
        self.linear1 = nn.Linear(d_model, self.d_ff)
        
        # ç¬¬äºŒå±‚ï¼šå‹ç¼©å› d_model
        self.linear2 = nn.Linear(self.d_ff, d_model)
        
        # Dropout
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        
        # æ¿€æ´»å‡½æ•°ï¼ˆTabM é€šå¸¸ä½¿ç”¨ GELUï¼‰
        self.activation = nn.GELU()
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features, d_model)
        Returns:
            out: (batch_size, num_features, d_model)
        """
        # æ‰©å±•ç»´åº¦
        x = self.linear1(x)  # (batch_size, num_features, d_ff)
        x = self.activation(x)
        x = self.dropout1(x)
        
        # å‹ç¼©å›åŸç»´åº¦
        x = self.linear2(x)  # (batch_size, num_features, d_model)
        x = self.dropout2(x)
        
        return x
```

### 4. é«˜çº§ FFN å˜ä½“

#### 4.1 é—¨æ§ FFN (Gated FFN)

```python
class GatedFFN(nn.Module):
    """é—¨æ§å‰é¦ˆç½‘ç»œï¼ˆç±»ä¼¼ GLUï¼‰"""
    def __init__(self, d_model, d_ff, dropout=0.0):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff * 2)  # è¾“å‡ºä¸¤å€ç»´åº¦
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        self.activation = nn.GELU()
    
    def forward(self, x):
        # ç¬¬ä¸€å±‚è¾“å‡ºåˆ†æˆä¸¤éƒ¨åˆ†
        gate_output = self.linear1(x)  # (..., d_ff * 2)
        gate, value = gate_output.chunk(2, dim=-1)  # åˆ†æˆä¸¤éƒ¨åˆ†
        
        # é—¨æ§æœºåˆ¶
        gated = self.activation(gate) * value  # å…ƒç´ çº§ä¹˜æ³•
        
        # ç¬¬äºŒå±‚
        output = self.linear2(gated)
        output = self.dropout(output)
        
        return output
```

#### 4.2 æ·±åº¦ FFN (Deep FFN)

```python
class DeepFFN(nn.Module):
    """å¤šå±‚å‰é¦ˆç½‘ç»œ"""
    def __init__(self, d_model, d_ff, num_layers=3, dropout=0.0):
        super().__init__()
        layers = []
        
        # ç¬¬ä¸€å±‚ï¼šæ‰©å±•åˆ° d_ff
        layers.append(nn.Linear(d_model, d_ff))
        layers.append(nn.GELU())
        layers.append(nn.Dropout(dropout))
        
        # ä¸­é—´å±‚ï¼šä¿æŒ d_ff ç»´åº¦
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(d_ff, d_ff))
            layers.append(nn.GELU())
            layers.append(nn.Dropout(dropout))
        
        # æœ€åä¸€å±‚ï¼šå‹ç¼©å› d_model
        layers.append(nn.Linear(d_ff, d_model))
        layers.append(nn.Dropout(dropout))
        
        self.net = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.net(x)
```

---

## æ¿€æ´»å‡½æ•°

### 1. ReLU (Rectified Linear Unit)

```python
ReLU(x) = max(0, x)
```

**ç‰¹ç‚¹**ï¼š
- âœ… è®¡ç®—ç®€å•å¿«é€Ÿ
- âœ… è§£å†³æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼ˆæ­£åŒºé—´ï¼‰
- âŒ æ­»äº¡ ReLU é—®é¢˜ï¼ˆè´ŸåŒºé—´æ¢¯åº¦ä¸º 0ï¼‰

**å®ç°**ï¼š
```python
class ReLU(nn.Module):
    def forward(self, x):
        return torch.maximum(x, torch.zeros_like(x))
```

### 2. GELU (Gaussian Error Linear Unit)

```python
GELU(x) = x * Î¦(x)
```

å…¶ä¸­ `Î¦(x)` æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

**è¿‘ä¼¼å…¬å¼**ï¼š
```python
GELU(x) â‰ˆ 0.5 * x * (1 + tanh(âˆš(2/Ï€) * (x + 0.044715 * xÂ³)))
```

**ç‰¹ç‚¹**ï¼š
- âœ… å¹³æ»‘çš„æ¿€æ´»å‡½æ•°
- âœ… åœ¨ Transformer ä¸­è¡¨ç°ä¼˜ç§€
- âœ… é¿å…æ­»äº¡ç¥ç»å…ƒé—®é¢˜

**å®ç°**ï¼š
```python
class GELU(nn.Module):
    def forward(self, x):
        return 0.5 * x * (1 + torch.tanh(
            torch.sqrt(torch.tensor(2.0 / 3.14159)) * 
            (x + 0.044715 * torch.pow(x, 3))
        ))
```

### 3. Swish / SiLU

```python
Swish(x) = x * sigmoid(x)
```

**ç‰¹ç‚¹**ï¼š
- âœ… å¹³æ»‘ä¸”å¯å¾®
- âœ… åœ¨æŸäº›ä»»åŠ¡ä¸Šæ¯” ReLU æ›´å¥½
- âœ… è‡ªé—¨æ§æœºåˆ¶

**å®ç°**ï¼š
```python
class Swish(nn.Module):
    def forward(self, x):
        return x * torch.sigmoid(x)
```

### 4. æ¿€æ´»å‡½æ•°å¯¹æ¯”

| æ¿€æ´»å‡½æ•° | å…¬å¼ | ä¼˜ç‚¹ | ç¼ºç‚¹ | ä½¿ç”¨åœºæ™¯ |
|---------|------|------|------|---------|
| **ReLU** | max(0, x) | ç®€å•å¿«é€Ÿ | æ­»äº¡ç¥ç»å…ƒ | é€šç”¨ |
| **GELU** | x * Î¦(x) | å¹³æ»‘ï¼Œæ€§èƒ½å¥½ | è®¡ç®—ç¨æ…¢ | Transformer |
| **Swish** | x * Ïƒ(x) | å¹³æ»‘ï¼Œè‡ªé—¨æ§ | è®¡ç®—ç¨æ…¢ | æŸäº›ä»»åŠ¡ |
| **Tanh** | tanh(x) | è¾“å‡ºèŒƒå›´ [-1, 1] | æ¢¯åº¦æ¶ˆå¤± | è¾ƒå°‘ä½¿ç”¨ |
| **Sigmoid** | 1/(1+eâ»Ë£) | è¾“å‡ºèŒƒå›´ [0, 1] | æ¢¯åº¦æ¶ˆå¤± | è¾“å‡ºå±‚ |

### 5. æ¿€æ´»å‡½æ•°å¯è§†åŒ–

```python
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-5, 5, 100)

# ReLU
relu = np.maximum(0, x)

# GELU (è¿‘ä¼¼)
gelu = 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))

# Swish
swish = x * (1 / (1 + np.exp(-x)))

# ç»˜å›¾
plt.figure(figsize=(12, 8))
plt.plot(x, relu, label='ReLU')
plt.plot(x, gelu, label='GELU')
plt.plot(x, swish, label='Swish')
plt.legend()
plt.grid(True)
plt.title('Activation Functions Comparison')
plt.show()
```

---

## æ­£åˆ™åŒ–æŠ€æœ¯

### 1. Dropout

**åŸç†**ï¼šè®­ç»ƒæ—¶éšæœºå°†éƒ¨åˆ†ç¥ç»å…ƒè¾“å‡ºç½®ä¸º 0ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆã€‚

```python
class Dropout(nn.Module):
    def __init__(self, p=0.5):
        super().__init__()
        self.p = p  # Dropout æ¦‚ç‡
    
    def forward(self, x):
        if self.training:
            # è®­ç»ƒæ—¶ï¼šéšæœºç½®é›¶
            mask = (torch.rand_like(x) > self.p).float()
            return x * mask / (1 - self.p)  # ç¼©æ”¾ä»¥ä¿æŒæœŸæœ›å€¼
        else:
            # æ¨ç†æ—¶ï¼šä¸åšä»»ä½•æ“ä½œ
            return x
```

**åœ¨ FFN ä¸­çš„ä½¿ç”¨**ï¼š
```python
class FFNWithDropout(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.activation = nn.GELU()
    
    def forward(self, x):
        x = self.linear1(x)
        x = self.activation(x)
        x = self.dropout1(x)  # ç¬¬ä¸€å±‚å Dropout
        x = self.linear2(x)
        x = self.dropout2(x)  # ç¬¬äºŒå±‚å Dropout
        return x
```

### 2. Layer Normalization

**åŸç†**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–ã€‚

```python
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))  # å¯å­¦ä¹ ç¼©æ”¾
        self.beta = nn.Parameter(torch.zeros(d_model))   # å¯å­¦ä¹ åç§»
        self.eps = eps
    
    def forward(self, x):
        # x: (batch_size, seq_len, d_model)
        mean = x.mean(dim=-1, keepdim=True)
        std = x.std(dim=-1, keepdim=True)
        normalized = (x - mean) / (std + self.eps)
        return self.gamma * normalized + self.beta
```

**åœ¨ Transformer Block ä¸­çš„ä½¿ç”¨**ï¼š
```python
class TransformerBlock(nn.Module):
    def __init__(self, d_model, d_ff, n_heads, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, n_heads)
        self.ffn = BasicFFN(d_model, d_ff, dropout=dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
    
    def forward(self, x):
        # è‡ªæ³¨æ„åŠ› + æ®‹å·® + å½’ä¸€åŒ–
        residual = x
        x = self.norm1(x)
        x = self.self_attn(x, x, x)[0]
        x = x + residual
        
        # FFN + æ®‹å·® + å½’ä¸€åŒ–
        residual = x
        x = self.norm2(x)
        x = self.ffn(x)
        x = x + residual
        
        return x
```

### 3. Batch Normalization vs Layer Normalization

| ç‰¹æ€§ | Batch Normalization | Layer Normalization |
|------|---------------------|---------------------|
| **å½’ä¸€åŒ–ç»´åº¦** | æ‰¹æ¬¡ç»´åº¦ | ç‰¹å¾ç»´åº¦ |
| **é€‚ç”¨åœºæ™¯** | CNN, å¤§æ‰¹æ¬¡ | Transformer, RNN |
| **è®­ç»ƒ/æ¨ç†** | éœ€è¦åŒºåˆ† | ä¸€è‡´ |
| **ä½ç½®** | é€šå¸¸åœ¨æ¿€æ´»å‰ | é€šå¸¸åœ¨æ¿€æ´»å |

**Batch Normalization**ï¼š
```python
# å¯¹æ‰¹æ¬¡ç»´åº¦å½’ä¸€åŒ–
# x: (batch_size, features)
mean = x.mean(dim=0)  # å¯¹æ‰¹æ¬¡ç»´åº¦æ±‚å‡å€¼
std = x.std(dim=0)
normalized = (x - mean) / (std + eps)
```

**Layer Normalization**ï¼š
```python
# å¯¹ç‰¹å¾ç»´åº¦å½’ä¸€åŒ–
# x: (batch_size, features)
mean = x.mean(dim=-1, keepdim=True)  # å¯¹ç‰¹å¾ç»´åº¦æ±‚å‡å€¼
std = x.std(dim=-1, keepdim=True)
normalized = (x - mean) / (std + eps)
```

### 4. æƒé‡è¡°å‡ (Weight Decay)

**åŸç†**ï¼šL2 æ­£åˆ™åŒ–ï¼Œåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„å¹³æ–¹å’Œã€‚

```python
# åœ¨ä¼˜åŒ–å™¨ä¸­è®¾ç½®
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01  # L2 æ­£åˆ™åŒ–ç³»æ•°
)
```

**æ•°å­¦è¡¨ç¤º**ï¼š
```
Loss = Original_Loss + Î» * Î£(wÂ²)
```

å…¶ä¸­ `Î»` æ˜¯ `weight_decay` å‚æ•°ã€‚

---

## å…¶ä»–ç›¸å…³æ¦‚å¿µ

### 1. æ®‹å·®è¿æ¥ (Residual Connection)

**åŸç†**ï¼šå°†è¾“å…¥ç›´æ¥åŠ åˆ°è¾“å‡ºä¸Šï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

```python
class ResidualFFN(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.0):
        super().__init__()
        self.ffn = BasicFFN(d_model, d_ff, dropout=dropout)
    
    def forward(self, x):
        residual = x
        x = self.ffn(x)
        return x + residual  # æ®‹å·®è¿æ¥
```

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼š
1. **æ¢¯åº¦æµåŠ¨**ï¼šæ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡æ®‹å·®è¿æ¥ä¼ æ’­
2. **èº«ä»½æ˜ å°„**ï¼šå¦‚æœ FFN å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„ï¼Œæ®‹å·®è¿æ¥ä¿è¯è‡³å°‘æ˜¯æ’ç­‰
3. **æ·±å±‚ç½‘ç»œ**ï¼šä½¿è®­ç»ƒæ›´æ·±çš„ç½‘ç»œæˆä¸ºå¯èƒ½

### 2. æ³¨æ„åŠ›æœºåˆ¶ä¸ FFN çš„é…åˆ

åœ¨ Transformer ä¸­ï¼Œæ³¨æ„åŠ›æœºåˆ¶å’Œ FFN é…åˆå·¥ä½œï¼š

```
è¾“å…¥ x
  â†“
Self-Attention: å­¦ä¹ ç‰¹å¾é—´çš„å…³ç³»
  â†“
Add & Norm: æ®‹å·®è¿æ¥ + å½’ä¸€åŒ–
  â†“
FFN: éçº¿æ€§å˜æ¢å’Œç‰¹å¾å¢å¼º
  â†“
Add & Norm: æ®‹å·®è¿æ¥ + å½’ä¸€åŒ–
  â†“
è¾“å‡º
```

**åˆ†å·¥**ï¼š
- **Self-Attention**ï¼šå­¦ä¹ "å“ªäº›ç‰¹å¾é‡è¦"ï¼ˆç‰¹å¾é€‰æ‹©ï¼‰
- **FFN**ï¼šå­¦ä¹ "å¦‚ä½•å˜æ¢ç‰¹å¾"ï¼ˆç‰¹å¾å˜æ¢ï¼‰

### 3. ä½ç½®ç¼–ç  (Positional Encoding)

è™½ç„¶ FFN æœ¬èº«ä¸æ¶‰åŠä½ç½®ç¼–ç ï¼Œä½†åœ¨ Transformer ä¸­ï¼Œä½ç½®ä¿¡æ¯å¾ˆé‡è¦ï¼š

```python
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len).unsqueeze(1).float()
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]
```

### 4. æ¢¯åº¦è£å‰ª (Gradient Clipping)

**åŸç†**ï¼šé™åˆ¶æ¢¯åº¦çš„å¤§å°ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚

```python
# æ–¹æ³• 1: æŒ‰èŒƒæ•°è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# æ–¹æ³• 2: æŒ‰å€¼è£å‰ª
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.5)
```

**åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨**ï¼š
```python
optimizer.zero_grad()
loss.backward()

# æ¢¯åº¦è£å‰ª
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

optimizer.step()
```

### 5. å­¦ä¹ ç‡è°ƒåº¦

**ReduceLROnPlateau**ï¼šå½“éªŒè¯æŸå¤±ä¸å†ä¸‹é™æ—¶é™ä½å­¦ä¹ ç‡ã€‚

```python
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='min',        # æœ€å°åŒ–æŒ‡æ ‡
    factor=0.5,        # å­¦ä¹ ç‡è¡°å‡å› å­
    patience=10,       # ç­‰å¾…è½®æ•°
    verbose=True
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
for epoch in range(epochs):
    train_loss = train_one_epoch()
    val_loss = validate()
    scheduler.step(val_loss)  # æ ¹æ®éªŒè¯æŸå¤±è°ƒæ•´
```

**CosineAnnealingLR**ï¼šä½™å¼¦é€€ç«è°ƒåº¦ã€‚

```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
    optimizer,
    T_max=100,  # æœ€å¤§å‘¨æœŸæ•°
    eta_min=1e-6  # æœ€å°å­¦ä¹ ç‡
)
```

---

## å®Œæ•´ä»£ç ç¤ºä¾‹

### TabM Block å®Œæ•´å®ç°ï¼ˆåŒ…å« FFNï¼‰

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

class TabMBlock(nn.Module):
    """å®Œæ•´çš„ TabM Blockï¼ŒåŒ…å«æ³¨æ„åŠ›æœºåˆ¶å’Œ FFN"""
    def __init__(self, d_model, n_heads=8, d_ff=None, dropout=0.0, tabm_k=32):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_ff = d_ff or (d_model * 4)
        self.tabm_k = tabm_k
        
        # 1. è‡ªæ³¨æ„åŠ›å±‚
        self.self_attn = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        
        # 2. FFN å±‚
        self.ffn = nn.Sequential(
            nn.Linear(d_model, self.d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(self.d_ff, d_model),
            nn.Dropout(dropout)
        )
        
        # 3. å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        
        # 4. Dropout
        self.dropout = nn.Dropout(dropout)
    
    def forward(self, x):
        """
        Args:
            x: (batch_size, num_features, d_model)
        Returns:
            out: (batch_size, num_features, d_model)
        """
        # ç¬¬ä¸€éƒ¨åˆ†ï¼šè‡ªæ³¨æ„åŠ› + æ®‹å·®è¿æ¥
        residual = x
        x = self.norm1(x)
        attn_out, _ = self.self_attn(x, x, x)
        x = residual + self.dropout(attn_out)
        
        # ç¬¬äºŒéƒ¨åˆ†ï¼šFFN + æ®‹å·®è¿æ¥
        residual = x
        x = self.norm2(x)
        ffn_out = self.ffn(x)
        x = residual + ffn_out
        
        return x


class TabMNet(nn.Module):
    """å®Œæ•´çš„ TabM ç½‘ç»œ"""
    def __init__(
        self,
        num_numeric,
        categorical_cardinalities,
        d_embedding=24,
        n_blocks=5,
        d_block=432,
        n_heads=8,
        dropout=0.0,
        tabm_k=32,
    ):
        super().__init__()
        
        # ç‰¹å¾åµŒå…¥ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦ PWL åµŒå…¥ï¼‰
        self.numeric_embedding = nn.Linear(num_numeric, d_embedding)
        if len(categorical_cardinalities) > 0:
            self.categorical_embedding = nn.ModuleList([
                nn.Embedding(card, d_embedding)
                for card in categorical_cardinalities
            ])
        else:
            self.categorical_embedding = None
        
        # è¾“å…¥æŠ•å½±
        num_features = num_numeric + len(categorical_cardinalities)
        self.input_projection = nn.Linear(d_embedding, d_block)
        
        # TabM Blocks
        self.blocks = nn.ModuleList([
            TabMBlock(d_block, n_heads, dropout=dropout, tabm_k=tabm_k)
            for _ in range(n_blocks)
        ])
        
        # å…¨å±€æ± åŒ–
        self.global_pool = nn.AdaptiveAvgPool1d(1)
        
        # è¾“å‡ºå±‚
        self.output_layer = nn.Sequential(
            nn.Linear(d_block, d_block // 2),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_block // 2, 1)
        )
    
    def forward(self, numeric_features, categorical_features=None):
        # ç‰¹å¾åµŒå…¥
        embedded = []
        
        # æ•°å€¼ç‰¹å¾
        if numeric_features is not None:
            numeric_emb = self.numeric_embedding(numeric_features)
            embedded.append(numeric_emb)
        
        # åˆ†ç±»ç‰¹å¾
        if self.categorical_embedding is not None and categorical_features is not None:
            cat_embs = []
            for i, emb in enumerate(self.categorical_embedding):
                cat_embs.append(emb(categorical_features[:, i]))
            cat_emb = torch.stack(cat_embs, dim=1)
            embedded.append(cat_emb)
        
        # æ‹¼æ¥
        if len(embedded) == 2:
            x = torch.cat(embedded, dim=1)
        elif len(embedded) == 1:
            x = embedded[0]
        else:
            raise ValueError("éœ€è¦è‡³å°‘ä¸€ç§ç‰¹å¾ç±»å‹")
        
        # æŠ•å½±
        x = self.input_projection(x)  # (batch_size, num_features, d_block)
        
        # TabM Blocks
        for block in self.blocks:
            x = block(x)
        
        # å…¨å±€æ± åŒ–
        x = x.transpose(1, 2)  # (batch_size, d_block, num_features)
        x = self.global_pool(x).squeeze(-1)  # (batch_size, d_block)
        
        # è¾“å‡º
        output = self.output_layer(x)
        return output.squeeze(-1)


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == '__main__':
    # åˆ›å»ºæ¨¡å‹
    model = TabMNet(
        num_numeric=8,
        categorical_cardinalities=[3, 4, 5],  # 3 ä¸ªåˆ†ç±»ç‰¹å¾
        d_embedding=24,
        n_blocks=5,
        d_block=432,
        n_heads=8,
        dropout=0.0,
        tabm_k=32,
    )
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    batch_size = 32
    numeric = torch.randn(batch_size, 8)
    categorical = torch.randint(0, 3, (batch_size, 3))
    
    # å‰å‘ä¼ æ’­
    output = model(numeric, categorical)
    print(f"Input shape: numeric={numeric.shape}, categorical={categorical.shape}")
    print(f"Output shape: {output.shape}")
```

---

## FFN è®¾è®¡åŸåˆ™

### 1. ç»´åº¦æ‰©å±•åŸåˆ™

**ä¸ºä»€ä¹ˆå…ˆæ‰©å±•å†å‹ç¼©ï¼Ÿ**

```
d_model â†’ d_ff (æ‰©å±•) â†’ d_model (å‹ç¼©)
```

**åŸå› **ï¼š
1. **è¡¨è¾¾èƒ½åŠ›**ï¼šæ›´å¤§çš„ä¸­é—´ç»´åº¦æä¾›æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›
2. **éçº¿æ€§å˜æ¢**ï¼šåœ¨æ›´å¤§çš„ç©ºé—´ä¸­è¿›è¡Œéçº¿æ€§å˜æ¢
3. **ä¿¡æ¯æµåŠ¨**ï¼šæ‰©å±•-å‹ç¼©çš„è¿‡ç¨‹ç±»ä¼¼äº"ç“¶é¢ˆ"ç»“æ„

### 2. æ¿€æ´»å‡½æ•°é€‰æ‹©

**åœ¨ FFN ä¸­é€šå¸¸ä½¿ç”¨**ï¼š
- **GELU**ï¼šTransformer ä¸­çš„æ ‡å‡†é€‰æ‹©
- **ReLU**ï¼šç®€å•å¿«é€Ÿï¼Œä½†å¯èƒ½ä¸å¦‚ GELU
- **Swish**ï¼šåœ¨æŸäº›ä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½

### 3. Dropout ä½ç½®

**é€šå¸¸çš„ä½ç½®**ï¼š
1. æ¿€æ´»å‡½æ•°ä¹‹å
2. çº¿æ€§å±‚ä¹‹å
3. æ®‹å·®è¿æ¥ä¹‹å‰ï¼ˆå¯é€‰ï¼‰

### 4. æ®‹å·®è¿æ¥çš„é‡è¦æ€§

**ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ**

1. **æ¢¯åº¦æµåŠ¨**ï¼šä½¿æ¢¯åº¦å¯ä»¥ç›´æ¥ä¼ æ’­
2. **èº«ä»½æ˜ å°„**ï¼šå¦‚æœ FFN å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„ï¼Œè‡³å°‘ä¿æŒåŸå€¼
3. **æ·±å±‚ç½‘ç»œ**ï¼šä½¿è®­ç»ƒæ·±å±‚ç½‘ç»œæˆä¸ºå¯èƒ½

---

## æ€§èƒ½ä¼˜åŒ–

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
with autocast():
    output = model(input)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. ä¼˜åŒ–å™¨é€‰æ‹©

**AdamW**ï¼ˆæ¨èï¼‰ï¼š
```python
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=0.001,
    weight_decay=0.01,
    betas=(0.9, 0.999)
)
```

**Adam**ï¼š
```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=0.001,
    betas=(0.9, 0.999)
)
```

---

## æ€»ç»“

### FFN çš„å…³é”®ç‚¹

1. **ç»“æ„**ï¼šæ‰©å±• â†’ æ¿€æ´» â†’ å‹ç¼©
2. **ä½œç”¨**ï¼šæä¾›éçº¿æ€§å˜æ¢å’Œç‰¹å¾å¢å¼º
3. **é…åˆ**ï¼šä¸æ³¨æ„åŠ›æœºåˆ¶é…åˆå·¥ä½œ
4. **æ­£åˆ™åŒ–**ï¼šä½¿ç”¨ Dropout å’Œ LayerNorm

### åœ¨ TabM ä¸­çš„ä½ç½®

```
è¾“å…¥ç‰¹å¾
  â†“
ç‰¹å¾åµŒå…¥ï¼ˆPWL + Categoricalï¼‰
  â†“
è¾“å…¥æŠ•å½±
  â†“
TabM Block 1
  â”œâ”€ Self-Attention
  â”œâ”€ Add & Norm
  â”œâ”€ FFN â† è¿™é‡Œ
  â””â”€ Add & Norm
  â†“
TabM Block 2-5
  ...
  â†“
å…¨å±€æ± åŒ–
  â†“
è¾“å‡ºå±‚
```

### ç›¸å…³æŠ€æœ¯æ ˆ

- **æ¿€æ´»å‡½æ•°**ï¼šGELU, ReLU, Swish
- **æ­£åˆ™åŒ–**ï¼šDropout, LayerNorm, Weight Decay
- **ä¼˜åŒ–æŠ€å·§**ï¼šæ®‹å·®è¿æ¥, æ¢¯åº¦è£å‰ª, å­¦ä¹ ç‡è°ƒåº¦
- **æ€§èƒ½ä¼˜åŒ–**ï¼šæ··åˆç²¾åº¦, ä¼˜åŒ–å™¨é€‰æ‹©

---

## å‚è€ƒèµ„æ–™

- **Transformer è®ºæ–‡**ï¼šAttention Is All You Need
- **GELU è®ºæ–‡**ï¼šGaussian Error Linear Units
- **Layer Normalization è®ºæ–‡**ï¼šLayer Normalization
- **PyTorch æ–‡æ¡£**ï¼šhttps://pytorch.org/docs/stable/index.html

