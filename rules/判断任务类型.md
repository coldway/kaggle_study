# åˆ¤æ–­ä»»åŠ¡ç±»å‹

## ğŸ“‹ ç›®å½•

1. [åˆ†ææµç¨‹æ¦‚è§ˆ](#åˆ†ææµç¨‹æ¦‚è§ˆ)
2. [ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜æè¿°](#ç¬¬ä¸€æ­¥ç†è§£é—®é¢˜æè¿°)
3. [ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹æ•°æ®æ–‡ä»¶](#ç¬¬äºŒæ­¥æŸ¥çœ‹æ•°æ®æ–‡ä»¶)
4. [ç¬¬ä¸‰æ­¥ï¼šåˆ†æç›®æ ‡å˜é‡](#ç¬¬ä¸‰æ­¥åˆ†æç›®æ ‡å˜é‡)
5. [ç¬¬å››æ­¥ï¼šç¡®å®šä»»åŠ¡ç±»å‹](#ç¬¬å››æ­¥ç¡®å®šä»»åŠ¡ç±»å‹)
6. [å…¶ä»–ä»»åŠ¡ç±»å‹è¯†åˆ«](#å…¶ä»–ä»»åŠ¡ç±»å‹è¯†åˆ«)
7. [å®Œæ•´åˆ†æç¤ºä¾‹](#å®Œæ•´åˆ†æç¤ºä¾‹)
8. [å¿«é€Ÿåˆ¤æ–­æ£€æŸ¥è¡¨](#å¿«é€Ÿåˆ¤æ–­æ£€æŸ¥è¡¨)

---

## åˆ†ææµç¨‹æ¦‚è§ˆ

### å®Œæ•´åˆ†ææµç¨‹

```
æ”¶åˆ°æ–°ä»»åŠ¡
  â”‚
  â”œâ”€ ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜æè¿°
  â”‚   â”œâ”€ é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
  â”‚   â”œâ”€ éœ€è¦é¢„æµ‹ä»€ä¹ˆï¼Ÿ
  â”‚   â””â”€ è¾“å‡ºæ˜¯ä»€ä¹ˆå½¢å¼ï¼Ÿ
  â”‚
  â”œâ”€ ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹æ•°æ®æ–‡ä»¶
  â”‚   â”œâ”€ æœ‰å“ªäº›æ–‡ä»¶ï¼Ÿ
  â”‚   â”œâ”€ æ•°æ®ç»“æ„å¦‚ä½•ï¼Ÿ
  â”‚   â””â”€ ç›®æ ‡å˜é‡æ˜¯ä»€ä¹ˆï¼Ÿ
  â”‚
  â”œâ”€ ç¬¬ä¸‰æ­¥ï¼šåˆ†æç›®æ ‡å˜é‡
  â”‚   â”œâ”€ æ•°æ®ç±»å‹
  â”‚   â”œâ”€ å”¯ä¸€å€¼æ•°é‡
  â”‚   â”œâ”€ å€¼åˆ†å¸ƒ
  â”‚   â””â”€ æ˜¯å¦æœ‰æ ‡ç­¾
  â”‚
  â””â”€ ç¬¬å››æ­¥ï¼šç¡®å®šä»»åŠ¡ç±»å‹
      â”œâ”€ å›å½’ä»»åŠ¡ï¼Ÿ
      â”œâ”€ åˆ†ç±»ä»»åŠ¡ï¼Ÿ
      â”œâ”€ å…¶ä»–ä»»åŠ¡ï¼Ÿ
      â””â”€ ç»¼åˆåˆ¤æ–­
```

---

## ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜æè¿°

### 1.1 é˜…è¯»é—®é¢˜æè¿°

**å…³é”®é—®é¢˜**ï¼š
- è¿™ä¸ªä»»åŠ¡è¦è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
- éœ€è¦é¢„æµ‹ä»€ä¹ˆï¼Ÿ
- è¾“å‡ºåº”è¯¥æ˜¯ä»€ä¹ˆå½¢å¼ï¼Ÿ

### 1.2 é—®é¢˜æè¿°ä¸­çš„å…³é”®è¯

#### å›å½’ä»»åŠ¡çš„å…³é”®è¯ âœ…

```
é¢„æµ‹æ•°å€¼ã€é¢„æµ‹ä»·æ ¼ã€é¢„æµ‹é£é™©ã€é¢„æµ‹å¾—åˆ†ã€é¢„æµ‹æ¸©åº¦ã€
é¢„æµ‹æ—¶é—´ã€é¢„æµ‹æ•°é‡ã€é¢„æµ‹æ¦‚ç‡ï¼ˆè¿ç»­ï¼‰ã€é¢„æµ‹æ¯”ä¾‹ã€
ä¼°è®¡ã€ä¼°ç®—ã€è®¡ç®—ã€è¯„ä¼°
```

**ç¤ºä¾‹é—®é¢˜**ï¼š
- "é¢„æµ‹æˆ¿å±‹ä»·æ ¼"
- "é¢„æµ‹é“è·¯äº‹æ•…é£é™©"
- "é¢„æµ‹ç”¨æˆ·è´­ä¹°é‡‘é¢"
- "é¢„æµ‹è‚¡ç¥¨ä»·æ ¼"
- "é¢„æµ‹æ¸©åº¦å˜åŒ–"

#### åˆ†ç±»ä»»åŠ¡çš„å…³é”®è¯ âœ…

```
åˆ†ç±»ã€è¯†åˆ«ã€åˆ¤æ–­ã€åŒºåˆ†ã€æ ‡æ³¨ã€æ ‡è®°ã€æ£€æµ‹ã€
é¢„æµ‹ç±»åˆ«ã€é¢„æµ‹æ ‡ç­¾ã€é¢„æµ‹ç±»å‹ã€é¢„æµ‹æ˜¯å¦ã€
é¢„æµ‹å“ªä¸ªã€é¢„æµ‹ç§ç±»
```

**ç¤ºä¾‹é—®é¢˜**ï¼š
- "åˆ¤æ–­é‚®ä»¶æ˜¯å¦ä¸ºåƒåœ¾é‚®ä»¶"
- "è¯†åˆ«å›¾åƒä¸­çš„ç‰©ä½“"
- "é¢„æµ‹ç”¨æˆ·æ˜¯å¦ä¼šè´­ä¹°"
- "åˆ†ç±»æ–°é—»æ–‡ç« "
- "è¯†åˆ«ç–¾ç—…ç±»å‹"

#### å…¶ä»–ä»»åŠ¡çš„å…³é”®è¯

```
èšç±»ã€åˆ†ç»„ã€æ¨èã€æ’åºã€ç”Ÿæˆã€ç¿»è¯‘ã€æ‘˜è¦
```

### 1.3 é—®é¢˜æè¿°åˆ†æä»£ç 

```python
def analyze_problem_description(description):
    """
    åˆ†æé—®é¢˜æè¿°ï¼Œåˆæ­¥åˆ¤æ–­ä»»åŠ¡ç±»å‹
    
    Args:
        description: é—®é¢˜æè¿°æ–‡æœ¬
    
    Returns:
        possible_types: å¯èƒ½çš„ä»»åŠ¡ç±»å‹åˆ—è¡¨
        keywords_found: æ‰¾åˆ°çš„å…³é”®è¯
    """
    description_lower = description.lower()
    
    # å›å½’ä»»åŠ¡å…³é”®è¯
    regression_keywords = [
        'é¢„æµ‹æ•°å€¼', 'é¢„æµ‹ä»·æ ¼', 'é¢„æµ‹é£é™©', 'é¢„æµ‹å¾—åˆ†',
        'é¢„æµ‹æ¸©åº¦', 'é¢„æµ‹æ—¶é—´', 'é¢„æµ‹æ•°é‡', 'é¢„æµ‹æ¦‚ç‡',
        'ä¼°è®¡', 'ä¼°ç®—', 'è®¡ç®—', 'è¯„ä¼°', 'predict value',
        'predict price', 'predict risk', 'predict score',
        'regression', 'å›å½’'
    ]
    
    # åˆ†ç±»ä»»åŠ¡å…³é”®è¯
    classification_keywords = [
        'åˆ†ç±»', 'è¯†åˆ«', 'åˆ¤æ–­', 'åŒºåˆ†', 'æ ‡æ³¨', 'æ ‡è®°',
        'é¢„æµ‹ç±»åˆ«', 'é¢„æµ‹æ ‡ç­¾', 'é¢„æµ‹ç±»å‹', 'é¢„æµ‹æ˜¯å¦',
        'classify', 'identify', 'detect', 'predict class',
        'predict label', 'predict type', 'classification'
    ]
    
    # èšç±»ä»»åŠ¡å…³é”®è¯
    clustering_keywords = [
        'èšç±»', 'åˆ†ç»„', 'cluster', 'group', 'segment'
    ]
    
    # æ¨èä»»åŠ¡å…³é”®è¯
    recommendation_keywords = [
        'æ¨è', 'recommend', 'recommendation'
    ]
    
    found_keywords = []
    possible_types = []
    
    # æ£€æŸ¥å…³é”®è¯
    for keyword in regression_keywords:
        if keyword in description_lower:
            found_keywords.append(keyword)
            if 'regression' not in possible_types:
                possible_types.append('regression')
    
    for keyword in classification_keywords:
        if keyword in description_lower:
            found_keywords.append(keyword)
            if 'classification' not in possible_types:
                possible_types.append('classification')
    
    for keyword in clustering_keywords:
        if keyword in description_lower:
            found_keywords.append(keyword)
            if 'clustering' not in possible_types:
                possible_types.append('clustering')
    
    for keyword in recommendation_keywords:
        if keyword in description_lower:
            found_keywords.append(keyword)
            if 'recommendation' not in possible_types:
                possible_types.append('recommendation')
    
    if not possible_types:
        possible_types.append('unknown')
    
    return possible_types, found_keywords

# ä½¿ç”¨ç¤ºä¾‹
description = "é¢„æµ‹é“è·¯äº‹æ•…é£é™©å€¼ï¼Œè¾“å‡º0-1ä¹‹é—´çš„è¿ç»­æ•°å€¼"
types, keywords = analyze_problem_description(description)
print(f"å¯èƒ½çš„ä»»åŠ¡ç±»å‹: {types}")
print(f"æ‰¾åˆ°çš„å…³é”®è¯: {keywords}")
# è¾“å‡º: å¯èƒ½çš„ä»»åŠ¡ç±»å‹: ['regression']
#      æ‰¾åˆ°çš„å…³é”®è¯: ['é¢„æµ‹é£é™©', 'predict value']
```

---

## ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹æ•°æ®æ–‡ä»¶

### 2.1 æ£€æŸ¥æ–‡ä»¶ç»“æ„

**éœ€è¦æŸ¥çœ‹çš„æ–‡ä»¶**ï¼š
- `train.csv` / `train.parquet` - è®­ç»ƒæ•°æ®
- `test.csv` / `test.parquet` - æµ‹è¯•æ•°æ®
- `sample_submission.csv` - æäº¤ç¤ºä¾‹
- `README.md` / `description.txt` - é—®é¢˜æè¿°

### 2.2 å¿«é€ŸæŸ¥çœ‹æ•°æ®

```python
import pandas as pd
import os

def explore_data_files(data_dir='.'):
    """
    æ¢ç´¢æ•°æ®æ–‡ä»¶ï¼Œäº†è§£æ•°æ®ç»“æ„
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
    
    Returns:
        data_info: æ•°æ®ä¿¡æ¯å­—å…¸
    """
    data_info = {
        'files': [],
        'train_shape': None,
        'test_shape': None,
        'columns': [],
        'target_column': None,
        'sample_submission': None
    }
    
    # æŸ¥æ‰¾æ•°æ®æ–‡ä»¶
    for file in os.listdir(data_dir):
        if file.endswith(('.csv', '.parquet')):
            data_info['files'].append(file)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_file = None
    for file in data_info['files']:
        if 'train' in file.lower():
            train_file = file
            break
    
    if train_file:
        if train_file.endswith('.csv'):
            train = pd.read_csv(os.path.join(data_dir, train_file), nrows=1000)
        else:
            train = pd.read_parquet(os.path.join(data_dir, train_file), nrows=1000)
        
        data_info['train_shape'] = train.shape
        data_info['columns'] = train.columns.tolist()
        
        # å°è¯•è¯†åˆ«ç›®æ ‡å˜é‡
        # å¸¸è§çš„ç›®æ ‡å˜é‡åç§°
        target_candidates = [
            'target', 'label', 'y', 'class', 'category',
            'price', 'value', 'score', 'risk', 'rating'
        ]
        
        for col in train.columns:
            if any(candidate in col.lower() for candidate in target_candidates):
                data_info['target_column'] = col
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå‡è®¾æœ€åä¸€åˆ—æ˜¯ç›®æ ‡å˜é‡
        if data_info['target_column'] is None:
            data_info['target_column'] = train.columns[-1]
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_file = None
    for file in data_info['files']:
        if 'test' in file.lower() and 'train' not in file.lower():
            test_file = file
            break
    
    if test_file:
        if test_file.endswith('.csv'):
            test = pd.read_csv(os.path.join(data_dir, test_file), nrows=100)
        else:
            test = pd.read_parquet(os.path.join(data_dir, test_file), nrows=100)
        data_info['test_shape'] = test.shape
    
    # æŸ¥çœ‹æäº¤ç¤ºä¾‹
    submission_file = None
    for file in data_info['files']:
        if 'submission' in file.lower() or 'sample' in file.lower():
            submission_file = file
            break
    
    if submission_file:
        if submission_file.endswith('.csv'):
            submission = pd.read_csv(os.path.join(data_dir, submission_file), nrows=10)
        else:
            submission = pd.read_parquet(os.path.join(data_dir, submission_file), nrows=10)
        data_info['sample_submission'] = submission
    
    return data_info

# ä½¿ç”¨ç¤ºä¾‹
data_info = explore_data_files('./playground-series-s5e10')
print("æ•°æ®æ–‡ä»¶ä¿¡æ¯:")
print(f"  æ–‡ä»¶åˆ—è¡¨: {data_info['files']}")
print(f"  è®­ç»ƒé›†å½¢çŠ¶: {data_info['train_shape']}")
print(f"  æµ‹è¯•é›†å½¢çŠ¶: {data_info['test_shape']}")
print(f"  åˆ—å: {data_info['columns']}")
print(f"  ç›®æ ‡å˜é‡: {data_info['target_column']}")
```

### 2.3 æŸ¥çœ‹æäº¤ç¤ºä¾‹

**æäº¤ç¤ºä¾‹æ–‡ä»¶éå¸¸é‡è¦**ï¼Œå®ƒå‘Šè¯‰ä½ ï¼š
- è¾“å‡ºæ ¼å¼æ˜¯ä»€ä¹ˆ
- éœ€è¦æäº¤ä»€ä¹ˆ

```python
def analyze_submission_format(submission_file):
    """
    åˆ†ææäº¤æ ¼å¼ï¼Œåˆ¤æ–­ä»»åŠ¡ç±»å‹
    
    Args:
        submission_file: æäº¤ç¤ºä¾‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        task_type: ä»»åŠ¡ç±»å‹
        format_info: æ ¼å¼ä¿¡æ¯
    """
    submission = pd.read_csv(submission_file)
    
    format_info = {
        'columns': submission.columns.tolist(),
        'dtypes': submission.dtypes.to_dict(),
        'sample_values': submission.head(5).to_dict(),
        'unique_values': {}
    }
    
    # åˆ†ææ¯ä¸€åˆ—
    for col in submission.columns:
        if col != 'id':
            format_info['unique_values'][col] = submission[col].nunique()
    
    # åˆ¤æ–­ä»»åŠ¡ç±»å‹
    target_cols = [col for col in submission.columns if col != 'id']
    
    if len(target_cols) == 1:
        target_col = target_cols[0]
        n_unique = format_info['unique_values'][target_col]
        dtype = format_info['dtypes'][target_col]
        
        if n_unique > 100 or dtype in ['float64', 'float32']:
            task_type = 'regression'
        elif n_unique == 2:
            task_type = 'binary_classification'
        elif n_unique <= 20:
            task_type = 'multi_classification'
        else:
            task_type = 'uncertain'
    else:
        task_type = 'multi_output'
    
    return task_type, format_info

# ä½¿ç”¨ç¤ºä¾‹
task_type, format_info = analyze_submission_format('sample_submission.csv')
print(f"ä»»åŠ¡ç±»å‹: {task_type}")
print(f"æ ¼å¼ä¿¡æ¯: {format_info}")
```

---

## ç¬¬ä¸‰æ­¥ï¼šåˆ†æç›®æ ‡å˜é‡

### 3.1 åŠ è½½å¹¶æŸ¥çœ‹ç›®æ ‡å˜é‡

```python
def analyze_target_variable(data_path, target_column=None):
    """
    è¯¦ç»†åˆ†æç›®æ ‡å˜é‡
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        target_column: ç›®æ ‡å˜é‡åˆ—åï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨è¯†åˆ«ï¼‰
    
    Returns:
        analysis_result: åˆ†æç»“æœå­—å…¸
    """
    # åŠ è½½æ•°æ®
    if data_path.endswith('.csv'):
        df = pd.read_csv(data_path)
    else:
        df = pd.read_parquet(data_path)
    
    # è¯†åˆ«ç›®æ ‡å˜é‡
    if target_column is None:
        # å°è¯•è‡ªåŠ¨è¯†åˆ«
        candidates = ['target', 'label', 'y', 'class']
        for col in df.columns:
            if any(c in col.lower() for c in candidates):
                target_column = col
                break
        
        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå‡è®¾æœ€åä¸€åˆ—
        if target_column is None:
            target_column = df.columns[-1]
    
    target = df[target_column]
    
    # åŸºæœ¬ç»Ÿè®¡
    analysis = {
        'column_name': target_column,
        'dtype': str(target.dtype),
        'n_total': len(target),
        'n_unique': target.nunique(),
        'unique_ratio': target.nunique() / len(target),
        'has_missing': target.isna().sum() > 0,
        'missing_count': target.isna().sum(),
        'sample_values': target.head(10).tolist(),
    }
    
    # æ•°å€¼ç‰¹å¾
    if target.dtype in ['int64', 'int32', 'float64', 'float32']:
        analysis['is_numeric'] = True
        analysis['min'] = target.min()
        analysis['max'] = target.max()
        analysis['mean'] = target.mean()
        analysis['std'] = target.std()
        analysis['has_decimals'] = (target % 1 != 0).any() if target.dtype in ['float64', 'float32'] else False
    else:
        analysis['is_numeric'] = False
        analysis['unique_values_list'] = target.unique().tolist()[:20]
    
    # å€¼åˆ†å¸ƒ
    value_counts = target.value_counts()
    analysis['value_counts'] = value_counts.head(10).to_dict()
    
    return analysis

# ä½¿ç”¨ç¤ºä¾‹
analysis = analyze_target_variable('train.csv', 'accident_risk')
print("ç›®æ ‡å˜é‡åˆ†æç»“æœ:")
for key, value in analysis.items():
    print(f"  {key}: {value}")
```

### 3.2 å¯è§†åŒ–ç›®æ ‡å˜é‡

```python
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_target_variable(target, save_path=None):
    """
    å¯è§†åŒ–ç›®æ ‡å˜é‡ï¼Œå¸®åŠ©åˆ¤æ–­ä»»åŠ¡ç±»å‹
    
    Args:
        target: ç›®æ ‡å˜é‡ï¼ˆSeriesï¼‰
        save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    """
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # 1. ç›´æ–¹å›¾
    axes[0, 0].hist(target.dropna(), bins=50, edgecolor='black')
    axes[0, 0].set_title('ç›®æ ‡å˜é‡åˆ†å¸ƒï¼ˆç›´æ–¹å›¾ï¼‰')
    axes[0, 0].set_xlabel('å€¼')
    axes[0, 0].set_ylabel('é¢‘æ•°')
    axes[0, 0].grid(True, alpha=0.3)
    
    # 2. ç®±çº¿å›¾
    axes[0, 1].boxplot(target.dropna())
    axes[0, 1].set_title('ç›®æ ‡å˜é‡åˆ†å¸ƒï¼ˆç®±çº¿å›¾ï¼‰')
    axes[0, 1].set_ylabel('å€¼')
    axes[0, 1].grid(True, alpha=0.3)
    
    # 3. å”¯ä¸€å€¼è®¡æ•°ï¼ˆå¦‚æœå”¯ä¸€å€¼ä¸å¤ªå¤šï¼‰
    n_unique = target.nunique()
    if n_unique <= 20:
        value_counts = target.value_counts()
        axes[1, 0].bar(range(len(value_counts)), value_counts.values)
        axes[1, 0].set_title(f'å”¯ä¸€å€¼é¢‘æ•°ï¼ˆå…±{n_unique}ä¸ªï¼‰')
        axes[1, 0].set_xlabel('å”¯ä¸€å€¼ç´¢å¼•')
        axes[1, 0].set_ylabel('é¢‘æ•°')
        axes[1, 0].grid(True, alpha=0.3)
    else:
        axes[1, 0].text(0.5, 0.5, 
                        f'å”¯ä¸€å€¼å¤ªå¤š({n_unique}ä¸ª)\nå¯èƒ½æ˜¯å›å½’ä»»åŠ¡',
                        ha='center', va='center', fontsize=14,
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        axes[1, 0].set_title('å”¯ä¸€å€¼åˆ†æ')
    
    # 4. ç´¯ç§¯åˆ†å¸ƒ
    sorted_values = np.sort(target.dropna())
    cumulative = np.arange(1, len(sorted_values) + 1) / len(sorted_values)
    axes[1, 1].plot(sorted_values, cumulative, linewidth=2)
    axes[1, 1].set_title('ç´¯ç§¯åˆ†å¸ƒå‡½æ•°')
    axes[1, 1].set_xlabel('å€¼')
    axes[1, 1].set_ylabel('ç´¯ç§¯æ¦‚ç‡')
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    
    plt.show()
    
    # æ‰“å°åˆ¤æ–­æç¤º
    print("\n" + "="*60)
    print("å¯è§†åŒ–åˆ†ææç¤º")
    print("="*60)
    if n_unique / len(target) > 0.5:
        print("âœ… å”¯ä¸€å€¼æ¯”ä¾‹é«˜ï¼Œå¯èƒ½æ˜¯å›å½’ä»»åŠ¡")
    elif n_unique <= 10:
        print("âœ… å”¯ä¸€å€¼æ•°é‡å°‘ï¼Œå¯èƒ½æ˜¯åˆ†ç±»ä»»åŠ¡")
        if n_unique == 2:
            print("   â†’ äºŒåˆ†ç±»ä»»åŠ¡")
        else:
            print(f"   â†’ å¤šåˆ†ç±»ä»»åŠ¡ï¼ˆ{n_unique}ä¸ªç±»åˆ«ï¼‰")
    else:
        print("âš ï¸  éœ€è¦ç»“åˆå…¶ä»–ä¿¡æ¯åˆ¤æ–­")

# ä½¿ç”¨ç¤ºä¾‹
train = pd.read_csv('train.csv')
target = train['accident_risk']
visualize_target_variable(target)
```

---

## ç¬¬å››æ­¥ï¼šç¡®å®šä»»åŠ¡ç±»å‹

### 4.1 ç»¼åˆåˆ¤æ–­å‡½æ•°

```python
def determine_task_type_comprehensive(
    problem_description=None,
    data_path=None,
    target_column=None,
    submission_file=None
):
    """
    ç»¼åˆåˆ¤æ–­ä»»åŠ¡ç±»å‹
    
    Args:
        problem_description: é—®é¢˜æè¿°æ–‡æœ¬
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        target_column: ç›®æ ‡å˜é‡åˆ—å
        submission_file: æäº¤ç¤ºä¾‹æ–‡ä»¶è·¯å¾„
    
    Returns:
        task_type: ä»»åŠ¡ç±»å‹
        confidence: ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        evidence: è¯æ®åˆ—è¡¨
    """
    evidence = []
    scores = {
        'regression': 0,
        'binary_classification': 0,
        'multi_classification': 0,
        'clustering': 0,
        'other': 0
    }
    
    # 1. ä»é—®é¢˜æè¿°åˆ¤æ–­
    if problem_description:
        types, keywords = analyze_problem_description(problem_description)
        if 'regression' in types:
            scores['regression'] += 2
            evidence.append(f"é—®é¢˜æè¿°åŒ…å«å›å½’å…³é”®è¯: {keywords}")
        if 'classification' in types:
            scores['multi_classification'] += 2
            evidence.append(f"é—®é¢˜æè¿°åŒ…å«åˆ†ç±»å…³é”®è¯: {keywords}")
    
    # 2. ä»ç›®æ ‡å˜é‡åˆ¤æ–­
    if data_path and target_column:
        analysis = analyze_target_variable(data_path, target_column)
        
        # å”¯ä¸€å€¼æ¯”ä¾‹
        unique_ratio = analysis['unique_ratio']
        if unique_ratio > 0.5:
            scores['regression'] += 3
            evidence.append(f"ç›®æ ‡å˜é‡å”¯ä¸€å€¼æ¯”ä¾‹é«˜({unique_ratio:.1%})")
        elif analysis['n_unique'] == 2:
            scores['binary_classification'] += 3
            evidence.append("ç›®æ ‡å˜é‡åªæœ‰2ä¸ªå”¯ä¸€å€¼ï¼ˆäºŒåˆ†ç±»ï¼‰")
        elif analysis['n_unique'] <= 20:
            scores['multi_classification'] += 2
            evidence.append(f"ç›®æ ‡å˜é‡å”¯ä¸€å€¼æ•°é‡å°‘({analysis['n_unique']}ä¸ª)")
        
        # æ•°æ®ç±»å‹
        if analysis['is_numeric'] and analysis.get('has_decimals', False):
            scores['regression'] += 2
            evidence.append("ç›®æ ‡å˜é‡æ˜¯è¿ç»­æ•°å€¼ï¼ˆæœ‰å°æ•°ï¼‰")
        elif not analysis['is_numeric']:
            scores['multi_classification'] += 2
            evidence.append("ç›®æ ‡å˜é‡æ˜¯ç±»åˆ«æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²ï¼‰")
    
    # 3. ä»æäº¤æ ¼å¼åˆ¤æ–­
    if submission_file:
        task_type_sub, format_info = analyze_submission_format(submission_file)
        if task_type_sub == 'regression':
            scores['regression'] += 2
            evidence.append("æäº¤æ ¼å¼æ˜¾ç¤ºä¸ºå›å½’ä»»åŠ¡")
        elif task_type_sub == 'binary_classification':
            scores['binary_classification'] += 2
            evidence.append("æäº¤æ ¼å¼æ˜¾ç¤ºä¸ºäºŒåˆ†ç±»ä»»åŠ¡")
        elif task_type_sub == 'multi_classification':
            scores['multi_classification'] += 2
            evidence.append("æäº¤æ ¼å¼æ˜¾ç¤ºä¸ºå¤šåˆ†ç±»ä»»åŠ¡")
    
    # ç¡®å®šä»»åŠ¡ç±»å‹
    max_score = max(scores.values())
    if max_score == 0:
        task_type = 'unknown'
        confidence = 0.0
        evidence.append("è¯æ®ä¸è¶³ï¼Œæ— æ³•åˆ¤æ–­")
    else:
        task_type = max(scores, key=scores.get)
        total_score = sum(scores.values())
        confidence = max_score / total_score if total_score > 0 else 0.0
    
    return task_type, confidence, evidence

# ä½¿ç”¨ç¤ºä¾‹
task_type, confidence, evidence = determine_task_type_comprehensive(
    problem_description="é¢„æµ‹é“è·¯äº‹æ•…é£é™©å€¼ï¼Œè¾“å‡º0-1ä¹‹é—´çš„è¿ç»­æ•°å€¼",
    data_path='train.csv',
    target_column='accident_risk',
    submission_file='sample_submission.csv'
)

print("="*60)
print("ä»»åŠ¡ç±»å‹åˆ¤æ–­ç»“æœ")
print("="*60)
print(f"ä»»åŠ¡ç±»å‹: {task_type}")
print(f"ç½®ä¿¡åº¦: {confidence:.1%}")
print(f"\nåˆ¤æ–­ä¾æ®:")
for i, ev in enumerate(evidence, 1):
    print(f"  {i}. {ev}")
print("="*60)
```

---

## å…¶ä»–ä»»åŠ¡ç±»å‹è¯†åˆ«

### 5.1 èšç±»ä»»åŠ¡

**ç‰¹å¾**ï¼š
- æ²¡æœ‰æ ‡ç­¾ï¼ˆæ— ç›‘ç£å­¦ä¹ ï¼‰
- ç›®æ ‡æ˜¯åˆ†ç»„æˆ–å‘ç°æ¨¡å¼
- è¾“å‡ºæ˜¯ç°‡æ ‡ç­¾

**åˆ¤æ–­æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥æ˜¯å¦æœ‰ç›®æ ‡å˜é‡
if 'target' not in df.columns and 'label' not in df.columns:
    # å¯èƒ½æ˜¯èšç±»ä»»åŠ¡
    print("å¯èƒ½æ˜¯èšç±»ä»»åŠ¡ï¼ˆæ— æ ‡ç­¾æ•°æ®ï¼‰")
```

### 5.2 æ¨èç³»ç»Ÿä»»åŠ¡

**ç‰¹å¾**ï¼š
- æœ‰ç”¨æˆ·IDå’Œç‰©å“ID
- ç›®æ ‡æ˜¯é¢„æµ‹è¯„åˆ†æˆ–æ¨è
- è¾“å‡ºæ˜¯è¯„åˆ†æˆ–æ’åº

**åˆ¤æ–­æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥åˆ—å
if 'user_id' in df.columns and 'item_id' in df.columns:
    # å¯èƒ½æ˜¯æ¨èä»»åŠ¡
    print("å¯èƒ½æ˜¯æ¨èç³»ç»Ÿä»»åŠ¡")
```

### 5.3 æ—¶é—´åºåˆ—ä»»åŠ¡

**ç‰¹å¾**ï¼š
- æœ‰æ—¶é—´æˆ³åˆ—
- ç›®æ ‡æ˜¯é¢„æµ‹æœªæ¥å€¼
- æ•°æ®æŒ‰æ—¶é—´æ’åº

**åˆ¤æ–­æ–¹æ³•**ï¼š
```python
# æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´åˆ—
time_columns = ['date', 'time', 'timestamp', 'datetime']
if any(col in df.columns for col in time_columns):
    # å¯èƒ½æ˜¯æ—¶é—´åºåˆ—ä»»åŠ¡
    print("å¯èƒ½æ˜¯æ—¶é—´åºåˆ—ä»»åŠ¡")
```

---

## å®Œæ•´åˆ†æç¤ºä¾‹

### ç¤ºä¾‹ï¼šåˆ†æä¸€ä¸ªæ–°ä»»åŠ¡

```python
def analyze_new_task(task_dir='.'):
    """
    å®Œæ•´åˆ†æä¸€ä¸ªæ–°ä»»åŠ¡
    
    Args:
        task_dir: ä»»åŠ¡ç›®å½•è·¯å¾„
    """
    print("="*70)
    print("æ–°ä»»åŠ¡åˆ†ææµç¨‹")
    print("="*70)
    
    # ç¬¬ä¸€æ­¥ï¼šæŸ¥æ‰¾å’Œé˜…è¯»é—®é¢˜æè¿°
    print("\nã€ç¬¬ä¸€æ­¥ã€‘æŸ¥æ‰¾é—®é¢˜æè¿°...")
    description_file = None
    for file in os.listdir(task_dir):
        if 'readme' in file.lower() or 'description' in file.lower():
            description_file = file
            break
    
    problem_description = None
    if description_file:
        with open(os.path.join(task_dir, description_file), 'r', encoding='utf-8') as f:
            problem_description = f.read()
        print(f"  æ‰¾åˆ°é—®é¢˜æè¿°æ–‡ä»¶: {description_file}")
        print(f"  é—®é¢˜æè¿°æ‘˜è¦: {problem_description[:200]}...")
    else:
        print("  âš ï¸  æœªæ‰¾åˆ°é—®é¢˜æè¿°æ–‡ä»¶")
    
    # ç¬¬äºŒæ­¥ï¼šæ¢ç´¢æ•°æ®æ–‡ä»¶
    print("\nã€ç¬¬äºŒæ­¥ã€‘æ¢ç´¢æ•°æ®æ–‡ä»¶...")
    data_info = explore_data_files(task_dir)
    print(f"  æ•°æ®æ–‡ä»¶: {data_info['files']}")
    print(f"  è®­ç»ƒé›†å½¢çŠ¶: {data_info['train_shape']}")
    print(f"  ç›®æ ‡å˜é‡: {data_info['target_column']}")
    
    # ç¬¬ä¸‰æ­¥ï¼šåˆ†æç›®æ ‡å˜é‡
    print("\nã€ç¬¬ä¸‰æ­¥ã€‘åˆ†æç›®æ ‡å˜é‡...")
    train_file = [f for f in data_info['files'] if 'train' in f.lower()][0]
    analysis = analyze_target_variable(
        os.path.join(task_dir, train_file),
        data_info['target_column']
    )
    print(f"  æ•°æ®ç±»å‹: {analysis['dtype']}")
    print(f"  å”¯ä¸€å€¼æ•°é‡: {analysis['n_unique']}")
    print(f"  å”¯ä¸€å€¼æ¯”ä¾‹: {analysis['unique_ratio']:.1%}")
    if analysis.get('has_decimals'):
        print(f"  åŒ…å«å°æ•°: æ˜¯")
    
    # å¯è§†åŒ–ç›®æ ‡å˜é‡
    train = pd.read_csv(os.path.join(task_dir, train_file))
    target = train[data_info['target_column']]
    print("\n  ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_target_variable(target)
    
    # ç¬¬å››æ­¥ï¼šæŸ¥çœ‹æäº¤æ ¼å¼
    print("\nã€ç¬¬å››æ­¥ã€‘æŸ¥çœ‹æäº¤æ ¼å¼...")
    submission_file = [f for f in data_info['files'] if 'submission' in f.lower() or 'sample' in f.lower()]
    if submission_file:
        task_type_sub, format_info = analyze_submission_format(
            os.path.join(task_dir, submission_file[0])
        )
        print(f"  æäº¤æ ¼å¼æ˜¾ç¤ºä»»åŠ¡ç±»å‹: {task_type_sub}")
        print(f"  æäº¤åˆ—: {format_info['columns']}")
    
    # ç¬¬äº”æ­¥ï¼šç»¼åˆåˆ¤æ–­
    print("\nã€ç¬¬äº”æ­¥ã€‘ç»¼åˆåˆ¤æ–­...")
    task_type, confidence, evidence = determine_task_type_comprehensive(
        problem_description=problem_description,
        data_path=os.path.join(task_dir, train_file),
        target_column=data_info['target_column'],
        submission_file=os.path.join(task_dir, submission_file[0]) if submission_file else None
    )
    
    print("\n" + "="*70)
    print("æœ€ç»ˆåˆ¤æ–­ç»“æœ")
    print("="*70)
    print(f"ä»»åŠ¡ç±»å‹: {task_type}")
    print(f"ç½®ä¿¡åº¦: {confidence:.1%}")
    print(f"\nåˆ¤æ–­ä¾æ®:")
    for i, ev in enumerate(evidence, 1):
        print(f"  {i}. {ev}")
    print("="*70)
    
    return task_type, confidence, evidence

# ä½¿ç”¨ç¤ºä¾‹
task_type, confidence, evidence = analyze_new_task('./playground-series-s5e10')
```

---

## å¿«é€Ÿåˆ¤æ–­æ£€æŸ¥è¡¨

### æ£€æŸ¥è¡¨

```
â–¡ ç¬¬ä¸€æ­¥ï¼šç†è§£é—®é¢˜
  â–¡ é˜…è¯»é—®é¢˜æè¿°
  â–¡ æ‰¾å‡ºå…³é”®è¯ï¼ˆé¢„æµ‹æ•°å€¼/é¢„æµ‹ç±»åˆ«ï¼‰
  â–¡ ç†è§£è¾“å‡ºè¦æ±‚

â–¡ ç¬¬äºŒæ­¥ï¼šæŸ¥çœ‹æ•°æ®
  â–¡ æ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶
  â–¡ æ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶
  â–¡ æ‰¾åˆ°æäº¤ç¤ºä¾‹æ–‡ä»¶
  â–¡ æŸ¥çœ‹æ•°æ®ç»“æ„

â–¡ ç¬¬ä¸‰æ­¥ï¼šåˆ†æç›®æ ‡å˜é‡
  â–¡ è¯†åˆ«ç›®æ ‡å˜é‡åˆ—
  â–¡ æŸ¥çœ‹æ•°æ®ç±»å‹
  â–¡ ç»Ÿè®¡å”¯ä¸€å€¼æ•°é‡
  â–¡ æ£€æŸ¥æ˜¯å¦æœ‰å°æ•°
  â–¡ å¯è§†åŒ–åˆ†å¸ƒ

â–¡ ç¬¬å››æ­¥ï¼šæŸ¥çœ‹æäº¤æ ¼å¼
  â–¡ æŸ¥çœ‹æäº¤æ–‡ä»¶åˆ—å
  â–¡ æŸ¥çœ‹æäº¤æ–‡ä»¶æ•°æ®ç±»å‹
  â–¡ æŸ¥çœ‹æäº¤æ–‡ä»¶ç¤ºä¾‹å€¼

â–¡ ç¬¬äº”æ­¥ï¼šç»¼åˆåˆ¤æ–­
  â–¡ å›å½’ä»»åŠ¡ï¼Ÿ
    - ç›®æ ‡å˜é‡æ˜¯è¿ç»­æ•°å€¼
    - å”¯ä¸€å€¼æ¯”ä¾‹é«˜
    - æœ‰å°æ•°éƒ¨åˆ†
    - ä½¿ç”¨RMSEç­‰å›å½’æŒ‡æ ‡
  â–¡ åˆ†ç±»ä»»åŠ¡ï¼Ÿ
    - ç›®æ ‡å˜é‡æ˜¯ç¦»æ•£ç±»åˆ«
    - å”¯ä¸€å€¼æ•°é‡å°‘ï¼ˆ2-100ä¸ªï¼‰
    - æ— å°æ•°éƒ¨åˆ†
    - ä½¿ç”¨Accuracyç­‰åˆ†ç±»æŒ‡æ ‡
  â–¡ å…¶ä»–ä»»åŠ¡ï¼Ÿ
    - æ— æ ‡ç­¾ â†’ èšç±»
    - æœ‰ç”¨æˆ·/ç‰©å“ â†’ æ¨è
    - æœ‰æ—¶é—´åºåˆ— â†’ æ—¶é—´åºåˆ—é¢„æµ‹
```

---

## æ€»ç»“

### åˆ†ææ–°ä»»åŠ¡çš„æ­¥éª¤

1. **ç†è§£é—®é¢˜æè¿°**
   - é˜…è¯»é—®é¢˜æè¿°
   - æ‰¾å‡ºå…³é”®è¯
   - ç†è§£è¾“å‡ºè¦æ±‚

2. **æŸ¥çœ‹æ•°æ®æ–‡ä»¶**
   - æ‰¾åˆ°è®­ç»ƒ/æµ‹è¯•æ•°æ®
   - æŸ¥çœ‹æ•°æ®ç»“æ„
   - è¯†åˆ«ç›®æ ‡å˜é‡

3. **åˆ†æç›®æ ‡å˜é‡**
   - æ•°æ®ç±»å‹
   - å”¯ä¸€å€¼ç»Ÿè®¡
   - å€¼åˆ†å¸ƒå¯è§†åŒ–

4. **æŸ¥çœ‹æäº¤æ ¼å¼**
   - æäº¤æ–‡ä»¶æ ¼å¼
   - è¾“å‡ºè¦æ±‚

5. **ç»¼åˆåˆ¤æ–­**
   - ç»“åˆæ‰€æœ‰è¯æ®
   - ç¡®å®šä»»åŠ¡ç±»å‹

### å¿«é€Ÿåˆ¤æ–­è§„åˆ™

| ç‰¹å¾ | å›å½’ä»»åŠ¡ | åˆ†ç±»ä»»åŠ¡ |
|------|---------|---------|
| **ç›®æ ‡å˜é‡** | è¿ç»­æ•°å€¼ | ç¦»æ•£ç±»åˆ« |
| **å”¯ä¸€å€¼æ¯”ä¾‹** | >50% | <10% |
| **æ•°æ®ç±»å‹** | float | int/string |
| **æœ‰å°æ•°** | æ˜¯ | å¦ |
| **è¯„ä¼°æŒ‡æ ‡** | RMSE/MAE | Accuracy/F1 |
| **æ¨¡å‹** | Regressor | Classifier |

### é‡åˆ°ä¸ç¡®å®šçš„æƒ…å†µ

1. **æŸ¥çœ‹æäº¤ç¤ºä¾‹**ï¼šè¿™æ˜¯æœ€å¯é çš„æ–¹æ³•
2. **æŸ¥çœ‹è¯„ä¼°æŒ‡æ ‡**ï¼šæŒ‡æ ‡ç±»å‹ç›´æ¥åæ˜ ä»»åŠ¡ç±»å‹
3. **æŸ¥çœ‹æ¨¡å‹ç±»å‹**ï¼šRegressor vs Classifier
4. **ç»¼åˆåˆ¤æ–­**ï¼šç»“åˆå¤šä¸ªè¯æ®

---

## å‚è€ƒèµ„æ–™

- **Kaggle ç«èµ›**ï¼šé—®é¢˜ç±»å‹è¯†åˆ«
- **æœºå™¨å­¦ä¹ åŸºç¡€**ï¼šä»»åŠ¡ç±»å‹åˆ†ç±»
- **æ•°æ®åˆ†æ**ï¼šæ¢ç´¢æ€§æ•°æ®åˆ†æ

